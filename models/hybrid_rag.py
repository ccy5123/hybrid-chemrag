# models/hybrid_rag.py
import json
import time
import logging
import numpy as np
import random
import anthropic
import re
import math
from typing import List, Dict, Tuple

# LangChain imports
try:
    from langchain.vectorstores import FAISS
    from langchain.embeddings import HuggingFaceEmbeddings
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.schema import Document
    LANGCHAIN_AVAILABLE = True
    print("âœ… LangChain loaded successfully!")
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ LangChain not available. Install with: pip install langchain sentence-transformers")

from .data_parser import DataParser
from .utils import (
    create_mol_object, generate_fingerprints, calculate_molecular_properties,
    calculate_multi_fingerprint_similarity, combine_similarity_scores
)
from .vector_store_manager import VectorStoreManager

logger = logging.getLogger(__name__)

class HybridSMILESRAG:
    """í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ: ìì—°ì–´(LangChain) + í™”í•™ì  ìœ ì‚¬ë„(RDKit)"""
    
    def __init__(self, claude_api_key: str = None, model: str = "claude-sonnet-4-20250514", temperature: float = 0.1):
        # Claude API ì„¤ì •
        import config
        self.config = config
        
        self.claude_client = anthropic.Anthropic(api_key=claude_api_key or config.CLAUDE_API_KEY)
        if not (claude_api_key or config.CLAUDE_API_KEY):
            raise ValueError("Claude API key not found. Set in config.py or pass as parameter.")
        
        self.model = model
        self.temperature = temperature
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.train_data = []
        self.parsed_train_data = []
        
        # ë²¡í„° ìŠ¤í† ì–´ ë§¤ë‹ˆì € ì´ˆê¸°í™” (ìƒˆë¡œ ì¶”ê°€!)
        if LANGCHAIN_AVAILABLE:
            self.vector_manager = VectorStoreManager(config)
            logger.info("ğŸ“ Vector store manager initialized")
        else:
            self.vector_manager = None
            logger.warning("âš ï¸ LangChain not available, vector store disabled")
        
        self.assay_vectorstore = None
        
        # RDKit ì»´í¬ë„ŒíŠ¸
        self.mol_objects = {}
        self.fingerprints = {}
        self.fingerprint_cache = {}  # ìƒˆë¡œ ì¶”ê°€!
        
        # ë¹„ìš© ì¶”ì 
        self.cost_tracker = {
            'input_tokens': 0,
            'output_tokens': 0,
            'total_cost': 0.0,
            'api_calls': 0
        }
        
        logger.info("ğŸ”¬ Hybrid RAG System initialized")
    
    def load_jsonl_data(self, file_path: str) -> List[Dict]:
        """JSONL íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        data = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    item = json.loads(line.strip())
                    data.append(item)
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            raise
        
        logger.info(f"Loaded {len(data)} samples from {file_path}")
        return data
    
    def simple_train_test_split(self, data: List[Dict], test_size: float = None, 
                               random_state: int = None) -> Tuple[List[Dict], List[Dict]]:
        """ê°„ë‹¨í•œ train/test ë¶„í• """
        from config import TEST_SIZE, RANDOM_STATE
        
        if test_size is None:
            test_size = TEST_SIZE
        if random_state is None:
            random_state = RANDOM_STATE
            
        random.seed(random_state)
        np.random.seed(random_state)
        
        shuffled_data = data.copy()
        random.shuffle(shuffled_data)
        
        split_idx = int(len(shuffled_data) * (1 - test_size))
        train_data = shuffled_data[:split_idx]
        test_data = shuffled_data[split_idx:]
        
        logger.info(f"Train set: {len(train_data)} samples")
        logger.info(f"Test set: {len(test_data)} samples")
        
        return train_data, test_data
    
    def prepare_hybrid_training_data(self, train_data: List[Dict]):
        """í•˜ì´ë¸Œë¦¬ë“œ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ - ë²¡í„° DB ìºì‹± í¬í•¨"""
        logger.info("ğŸ—ï¸ Preparing hybrid training data with caching...")
        
        self.train_data = train_data
        self.parsed_train_data = []
        
        # ë²¡í„°ìŠ¤í† ì–´ ì •ë³´ ì¶œë ¥
        if self.vector_manager:
            info = self.vector_manager.get_vectorstore_info()
            logger.info("ğŸ“Š Vector Store Status:")
            logger.info(f"   Assay vectorstore exists: {info['assay_vectorstore_exists']}")
            logger.info(f"   Fingerprint cache exists: {info['fingerprint_cache_exists']}")
            
            if info['assay_vectorstore_exists']:
                logger.info(f"   Vectorstore size: {info.get('vectorstore_size_mb', 0):.1f} MB")
            if info['fingerprint_cache_exists']:
                logger.info(f"   Cache size: {info.get('cache_size_mb', 0):.1f} MB")
        
        # 1. ê¸°ë³¸ ë°ì´í„° íŒŒì‹±
        for idx, item in enumerate(train_data):
            parsed = DataParser.parse_input_text(item['input_text'])
            parsed['logac50'] = int(item['output_text'])
            parsed['idx'] = idx
            parsed['activity_category'] = self._categorize_activity(parsed['logac50'])
            self.parsed_train_data.append(parsed)
            
            if (idx + 1) % 100 == 0:
                logger.info(f"   Parsed {idx + 1}/{len(train_data)} samples...")
        
        # 2. Assay ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ/ìƒì„±
        if LANGCHAIN_AVAILABLE and self.vector_manager:
            self.assay_vectorstore = self.vector_manager.load_or_create_assay_vectorstore(train_data)
        
        # 3. ë¶„ì ì§€ë¬¸ ìºì‹œ ë¡œë“œ/ìƒì„±
        if self.vector_manager:
            self.fingerprint_cache = self.vector_manager.load_or_create_fingerprint_cache(self.parsed_train_data)
            
            # íŒŒì‹±ëœ ë°ì´í„°ì— ìºì‹œëœ ì§€ë¬¸ ì ìš©
            for parsed in self.parsed_train_data:
                if parsed['smiles'] in self.fingerprint_cache:
                    parsed['fingerprints'] = self.fingerprint_cache[parsed['smiles']]
                    
                    # ë¶„ì ê°ì²´ë„ ìƒì„± (í•„ìš”ì‹œ)
                    if parsed['smiles'] not in self.mol_objects:
                        mol = create_mol_object(parsed['smiles'])
                        if mol is not None:
                            self.mol_objects[parsed['smiles']] = mol
                            parsed['mol'] = mol
                            parsed['molecular_props'] = calculate_molecular_properties(mol)
        
        logger.info(f"âœ… Prepared {len(self.parsed_train_data)} training examples")
        
        if self.vector_manager:
            # ìµœì¢… í†µê³„
            cached_fingerprints = len(self.fingerprint_cache)
            vectorstore_docs = self.assay_vectorstore.index.ntotal if self.assay_vectorstore else 0
            
            logger.info("ğŸ“ˆ Caching Statistics:")
            logger.info(f"   Cached fingerprints: {cached_fingerprints}")
            logger.info(f"   Vectorstore documents: {vectorstore_docs}")
    
    def _categorize_activity(self, logac50: int) -> str:
        """í™œì„±ë„ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        if logac50 >= 85:
            return "Very High"
        elif logac50 >= 70:
            return "High"
        elif logac50 >= 50:
            return "Medium"
        elif logac50 >= 30:
            return "Low"
        else:
            return "Very Low"
    
    def hybrid_similarity_search(self, query_input: str, k_assay: int = None, k_chemical: int = None) -> Tuple[List[Dict], List[Dict]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        from config import K_ASSAY, K_CHEMICAL
        
        if k_assay is None:
            k_assay = K_ASSAY
        if k_chemical is None:
            k_chemical = K_CHEMICAL
            
        parsed_query = DataParser.parse_input_text(query_input)
        
        # ì‹¤í—˜ ì¡°ê±´ ìœ ì‚¬ë„ ê²€ìƒ‰
        similar_assays = []
        if LANGCHAIN_AVAILABLE and self.assay_vectorstore and parsed_query['assay_description']:
            try:
                assay_query = f"{parsed_query['assay_name']} {parsed_query['assay_description']}"
                assay_docs = self.assay_vectorstore.similarity_search_with_score(assay_query, k=k_assay)
                
                for doc, score in assay_docs:
                    similar_assays.append({
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'similarity_score': 1 - score,
                        'assay_name': doc.metadata.get('assay_name', ''),
                        'logac50': doc.metadata.get('logac50', 0)
                    })
                    
                logger.debug(f"Found {len(similar_assays)} similar assays")
            except Exception as e:
                logger.warning(f"Assay search failed: {e}")
        
        # í™”í•™ì  ìœ ì‚¬ë„ ê²€ìƒ‰
        similar_molecules = []
        if parsed_query['smiles']:
            try:
                query_mol = create_mol_object(parsed_query['smiles'])
                if query_mol is not None:
                    query_fps = generate_fingerprints(query_mol, parsed_query['smiles'])
                    
                    similarities = []
                    for example in self.parsed_train_data:
                        if 'fingerprints' in example and example['fingerprints']:
                            from config import FINGERPRINT_WEIGHTS
                            similarity_scores = calculate_multi_fingerprint_similarity(
                                query_fps, example['fingerprints']
                            )
                            final_similarity = combine_similarity_scores(similarity_scores, FINGERPRINT_WEIGHTS)
                            similarities.append((final_similarity, example, similarity_scores))
                    
                    similarities.sort(key=lambda x: x[0], reverse=True)
                    
                    for sim_score, example, breakdown in similarities[:k_chemical]:
                        similar_molecules.append({
                            'smiles': example['smiles'],
                            'logac50': example['logac50'],
                            'activity_category': example.get('activity_category', ''),
                            'molecular_props': example.get('molecular_props', {}),
                            'similarity_score': sim_score,
                            'similarity_breakdown': breakdown,
                            'assay_name': example.get('assay_name', '')
                        })
                        
                    logger.debug(f"Found {len(similar_molecules)} similar molecules")
            except Exception as e:
                logger.warning(f"Chemical search failed: {e}")
        
        return similar_assays, similar_molecules
    
    def calculate_context_weights(self, similar_assays: List[Dict], similar_molecules: List[Dict]) -> Dict:
        """ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ë™ì  ê³„ì‚°"""
        from config import K_ASSAY, K_CHEMICAL
        
        max_assay_sim = max([assay.get('similarity_score', 0) for assay in similar_assays]) if similar_assays else 0
        max_chem_sim = max([mol.get('similarity_score', 0) for mol in similar_molecules]) if similar_molecules else 0
        
        assay_availability = len(similar_assays) / K_ASSAY
        chem_availability = len(similar_molecules) / K_CHEMICAL
        
        # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        if max_assay_sim > 0.8 and max_chem_sim < 0.5:
            weights = {'assay': 0.75, 'chemical': 0.25}
        elif max_chem_sim > 0.8 and max_assay_sim < 0.5:
            weights = {'assay': 0.25, 'chemical': 0.75}
        elif max_assay_sim > 0.7 and max_chem_sim > 0.7:
            weights = {'assay': 0.5, 'chemical': 0.5}
        else:
            base_assay_weight = 0.4 + (assay_availability * 0.2)
            base_chem_weight = 0.6 - (assay_availability * 0.2)
            weights = {'assay': base_assay_weight, 'chemical': base_chem_weight}
        
        # ì •ê·œí™”
        total = weights['assay'] + weights['chemical']
        weights = {k: v/total for k, v in weights.items()}
        
        logger.debug(f"Context weights: Assay={weights['assay']:.2f}, Chemical={weights['chemical']:.2f}")
        
        return weights
    
    def create_hybrid_prompt(self, query_input: str, similar_assays: List[Dict], 
                           similar_molecules: List[Dict], weights: Dict) -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸ í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        parsed_query = DataParser.parse_input_text(query_input)
        
        # ì‹¤í—˜ ì¡°ê±´ ì»¨í…ìŠ¤íŠ¸
        assay_context = ""
        if similar_assays:
            assay_context = f"ğŸ§ª EXPERIMENTAL PROTOCOL CONTEXT (Weight: {weights['assay']:.2f}):\n\n"
            for i, assay in enumerate(similar_assays, 1):
                assay_context += f"Similar Assay {i} (Similarity: {assay['similarity_score']:.3f}):\n"
                assay_context += f"  {assay['content']}\n\n"
        else:
            assay_context = "ğŸ§ª EXPERIMENTAL PROTOCOL CONTEXT: No similar assays found.\n\n"
        
        # í™”í•™ êµ¬ì¡° ì»¨í…ìŠ¤íŠ¸
        chemical_context = ""
        if similar_molecules:
            chemical_context = f"ğŸ§¬ CHEMICAL STRUCTURE CONTEXT (Weight: {weights['chemical']:.2f}):\n\n"
            for i, mol in enumerate(similar_molecules, 1):
                chemical_context += f"Similar Molecule {i} (Tanimoto: {mol['similarity_score']:.3f}):\n"
                chemical_context += f"  SMILES: {mol['smiles']}\n"
                chemical_context += f"  LogAC50: {mol['logac50']}\n"
                chemical_context += f"  Activity: {mol['activity_category']}\n"
                
                if 'similarity_breakdown' in mol:
                    breakdown = mol['similarity_breakdown']
                    chemical_context += f"  Fingerprint Details:\n"
                    chemical_context += f"    - Morgan: {breakdown.get('morgan', 0):.3f}\n"
                    chemical_context += f"    - MACCS: {breakdown.get('maccs', 0):.3f}\n"
                    chemical_context += f"    - RDKit: {breakdown.get('rdkit', 0):.3f}\n"
                
                if 'molecular_props' in mol and mol['molecular_props']:
                    props = mol['molecular_props']
                    chemical_context += f"  Properties: MW={props.get('mw', 'N/A'):.1f}, "
                    chemical_context += f"LogP={props.get('logp', 'N/A'):.2f}\n"
                
                chemical_context += "\n"
        else:
            chemical_context = "ğŸ§¬ CHEMICAL STRUCTURE CONTEXT: No similar molecules found.\n\n"
        
        # í†µí•© í”„ë¡¬í”„íŠ¸
        integrated_prompt = f"""<thinking>
I am performing a hybrid analysis combining experimental protocol knowledge and chemical structure similarity for toxicity prediction.

Query Details:
- Assay: {parsed_query['assay_name']}
- SMILES: {parsed_query['smiles']}

Context Analysis:
- Assay context weight: {weights['assay']:.2f}
- Chemical context weight: {weights['chemical']:.2f}

This weighting suggests I should prioritize {"experimental context" if weights['assay'] > weights['chemical'] else "chemical structure analysis"} while considering both sources of information.

Let me analyze the patterns systematically...
</thinking>

{assay_context}

{chemical_context}

ğŸ¯ HYBRID TOXICITY PREDICTION TASK:

Query Input:
- Assay: {parsed_query['assay_name']}
- SMILES: {parsed_query['smiles']}
- Task: {parsed_query['instruction']}

ğŸ“Š INTEGRATED ANALYSIS FRAMEWORK:

1. **Context Weighting Strategy**:
   - Experimental Protocol Weight: {weights['assay']:.2f}
   - Chemical Structure Weight: {weights['chemical']:.2f}

2. **Primary Analysis Focus**:
   {"Focus on experimental protocol patterns and assay-specific factors" if weights['assay'] > 0.6 else "Focus on chemical structure-activity relationships" if weights['chemical'] > 0.6 else "Balance both experimental and chemical contexts equally"}

3. **Cross-Validation Approach**:
   - Compare patterns from both experimental and chemical contexts
   - Identify consistent vs conflicting predictions
   - Resolve conflicts using the higher-weighted context

4. **Evidence Integration**:
   - Experimental evidence: {"Strong" if len(similar_assays) >= 2 else "Moderate" if len(similar_assays) == 1 else "Weak"}
   - Chemical evidence: {"Strong" if len(similar_molecules) >= 3 else "Moderate" if len(similar_molecules) >= 1 else "Weak"}

ğŸ”¬ REQUIRED ANALYSIS:

**EXPERIMENTAL CONTEXT ANALYSIS**:
[Analyze the experimental protocol patterns and assay-specific factors]

**CHEMICAL STRUCTURE ANALYSIS**:
[Analyze the molecular structure and chemical similarity patterns]

**INTEGRATED PREDICTION LOGIC**:
[Combine both contexts using the calculated weights]

**FINAL PREDICTION**: [INTEGER 0-100]

**CONFIDENCE ASSESSMENT**: [High/Medium/Low with justification based on context quality]

Remember: Weight your analysis according to the calculated context weights, but always provide reasoning from both experimental and chemical perspectives when available."""

        return integrated_prompt
    
    def predict_single(self, query_input: str) -> Tuple[int, str, float, Dict]:
        """ë‹¨ì¼ ì…ë ¥ì— ëŒ€í•œ í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡"""
        start_time = time.time()
        
        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ìœ ì‚¬ë„ ê²€ìƒ‰
            similar_assays, similar_molecules = self.hybrid_similarity_search(query_input)
            
            # ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ê³„ì‚°
            weights = self.calculate_context_weights(similar_assays, similar_molecules)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.create_hybrid_prompt(query_input, similar_assays, similar_molecules, weights)
            
            # Claude API í˜¸ì¶œ
            from config import MAX_TOKENS
            response = self.claude_client.messages.create(
                model=self.model,
                max_tokens=MAX_TOKENS,
                temperature=self.temperature,
                messages=[{"role": "user", "content": prompt}]
            )
            
            result_text = response.content[0].text
            
            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
            self.cost_tracker['input_tokens'] += response.usage.input_tokens
            self.cost_tracker['output_tokens'] += response.usage.output_tokens
            self.cost_tracker['api_calls'] += 1
            
            # ì˜ˆì¸¡ê°’ ì¶”ì¶œ
            prediction = self._extract_prediction(result_text)
            elapsed_time = time.time() - start_time
            
            # ë©”íƒ€ë°ì´í„°
            metadata = {
                'weights': weights,
                'n_similar_assays': len(similar_assays),
                'n_similar_molecules': len(similar_molecules),
                'max_assay_similarity': max([a.get('similarity_score', 0) for a in similar_assays]) if similar_assays else 0,
                'max_chemical_similarity': max([m.get('similarity_score', 0) for m in similar_molecules]) if similar_molecules else 0
            }
            
            logger.debug(f"Hybrid prediction: {prediction} (assay_weight: {weights['assay']:.2f}, chem_weight: {weights['chemical']:.2f})")
            
            return prediction, result_text, elapsed_time, metadata
            
        except Exception as e:
            logger.error(f"Error in hybrid prediction: {e}")
            return 50, f"Error: {str(e)}", 0.0, {}
    
    def _extract_prediction(self, result_text: str) -> int:
        """LLM ì‘ë‹µì—ì„œ ì˜ˆì¸¡ê°’ ì¶”ì¶œ"""
        patterns = [
            r'FINAL PREDICTION:\s*(\d{1,3})',
            r'PREDICTION:\s*(\d{1,3})',
            r'Prediction:\s*(\d{1,3})',
            r'LogAC50:\s*(\d{1,3})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, result_text, re.IGNORECASE)
            if match:
                val = int(match.group(1))
                if 0 <= val <= 100:
                    return val
        
        # ë°±ì—…: 0-100 ë²”ìœ„ì˜ ì²« ë²ˆì§¸ ìˆ«ì
        numbers = re.findall(r'\b(\d{1,3})\b', result_text)
        for num in numbers:
            val = int(num)
            if 0 <= val <= 100:
                return val
        
        logger.warning("Could not extract valid prediction, using default value 50")
        return 50
    
    def calculate_cost(self) -> Dict:
        """ë¹„ìš© ê³„ì‚°"""
        from config import INPUT_COST_PER_1K, OUTPUT_COST_PER_1K
        
        input_cost = (self.cost_tracker['input_tokens'] / 1000) * INPUT_COST_PER_1K
        output_cost = (self.cost_tracker['output_tokens'] / 1000) * OUTPUT_COST_PER_1K
        total_cost = input_cost + output_cost
        
        return {
            'input_tokens': self.cost_tracker['input_tokens'],
            'output_tokens': self.cost_tracker['output_tokens'],
            'api_calls': self.cost_tracker['api_calls'],
            'input_cost': input_cost,
            'output_cost': output_cost,
            'total_cost': total_cost
        }