{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain sentence-transformers\n",
    "# !conda install -c conda-forge rdkit\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anthropic\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import logging\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# LangChain imports (ë¡œì»¬ ì„ë² ë”© ì‚¬ìš©)\n",
    "try:\n",
    "    from langchain.vectorstores import FAISS\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.schema import Document\n",
    "    LANGCHAIN_AVAILABLE = True\n",
    "    print(\"âœ… LangChain loaded successfully!\")\n",
    "except ImportError:\n",
    "    LANGCHAIN_AVAILABLE = False\n",
    "    print(\"âš ï¸ LangChain not available. Install with: pip install langchain sentence-transformers\")\n",
    "\n",
    "# RDKit imports for molecular similarity\n",
    "try:\n",
    "    from rdkit import Chem, DataStructs\n",
    "    from rdkit.Chem import AllChem, MACCSkeys, Descriptors\n",
    "    from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "    RDKIT_AVAILABLE = True\n",
    "    print(\"âœ… RDKit loaded successfully!\")\n",
    "except ImportError:\n",
    "    RDKIT_AVAILABLE = False\n",
    "    print(\"âš ï¸ RDKit not available. Install with: conda install -c conda-forge rdkit\")\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataParser:\n",
    "    \"\"\"ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì‹¤í—˜ ì¡°ê±´ê³¼ SMILESë¡œ ë¶„ë¦¬\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_input_text(input_text: str) -> Dict:\n",
    "        \"\"\"input_textë¥¼ êµ¬ì„± ìš”ì†Œë¡œ ë¶„ë¦¬\"\"\"\n",
    "        try:\n",
    "            # SMILES ì¶”ì¶œ\n",
    "            smiles_pattern = r'SMILES:\\s*([^\\n\\r]+)'\n",
    "            smiles_match = re.search(smiles_pattern, input_text)\n",
    "            smiles = smiles_match.group(1).strip() if smiles_match else \"\"\n",
    "            \n",
    "            # Assay ì´ë¦„ ì¶”ì¶œ\n",
    "            assay_pattern = r'Assay:\\s*([^\\n\\r]+)'\n",
    "            assay_match = re.search(assay_pattern, input_text)\n",
    "            assay_name = assay_match.group(1).strip() if assay_match else \"\"\n",
    "            \n",
    "            # ì‹¤í—˜ ì„¤ëª… ì¶”ì¶œ (TOX21... ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„)\n",
    "            assay_desc_pattern = r'(TOX21_[^\\.]+[^\\.]*\\.)'\n",
    "            assay_desc_match = re.search(assay_desc_pattern, input_text, re.DOTALL)\n",
    "            assay_description = assay_desc_match.group(1).strip() if assay_desc_match else \"\"\n",
    "            \n",
    "            # ì „ì²´ ì§€ì‹œì‚¬í•­ ì¶”ì¶œ\n",
    "            instruction_pattern = r'(Given an Assay and SMILES.*?)(?:SMILES:|$)'\n",
    "            instruction_match = re.search(instruction_pattern, input_text, re.DOTALL)\n",
    "            instruction = instruction_match.group(1).strip() if instruction_match else \"\"\n",
    "            \n",
    "            return {\n",
    "                'smiles': smiles,\n",
    "                'assay_name': assay_name,\n",
    "                'assay_description': assay_description,\n",
    "                'instruction': instruction,\n",
    "                'full_text': input_text\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing input text: {e}\")\n",
    "            return {\n",
    "                'smiles': '',\n",
    "                'assay_name': '',\n",
    "                'assay_description': '',\n",
    "                'instruction': '',\n",
    "                'full_text': input_text\n",
    "            }\n",
    "\n",
    "class HybridSMILESRAG:\n",
    "    \"\"\"\n",
    "    í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ: ìì—°ì–´(LangChain) + í™”í•™ì  ìœ ì‚¬ë„(RDKit)\n",
    "    \n",
    "    Features:\n",
    "    - ì‹¤í—˜ ì¡°ê±´ ìœ ì‚¬ë„: LangChain + ë¡œì»¬ ì„ë² ë”©\n",
    "    - í™”í•™ì  ìœ ì‚¬ë„: RDKit ë¶„ì ì§€ë¬¸\n",
    "    - ì ì‘í˜• ìœµí•©: ë™ì  ê°€ì¤‘ì¹˜ ê¸°ë°˜ í†µí•©\n",
    "    - Claude 3.7 ì¶”ë¡ : í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸ í™œìš©\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, claude_api_key: str = None, model: str = \"claude-sonnet-4-20250514\", temperature: float = 0.1):\n",
    "        # Claude API ì„¤ì •\n",
    "        self.claude_client = anthropic.Anthropic(\n",
    "            api_key=claude_api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        )\n",
    "        if not (claude_api_key or os.getenv(\"ANTHROPIC_API_KEY\")):\n",
    "            raise ValueError(\"Claude API key not found. Set ANTHROPIC_API_KEY environment variable.\")\n",
    "        \n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # ë°ì´í„° ì €ì¥ì†Œ\n",
    "        self.train_data = []\n",
    "        self.parsed_train_data = []\n",
    "        \n",
    "        # LangChain ì»´í¬ë„ŒíŠ¸ (ë¡œì»¬ ì„ë² ë”© ì‚¬ìš©)\n",
    "        if LANGCHAIN_AVAILABLE:\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"all-MiniLM-L6-v2\",\n",
    "                model_kwargs={'device': 'cpu'}\n",
    "            )\n",
    "            self.assay_vectorstore = None\n",
    "            logger.info(\"ğŸ”¤ Using local HuggingFace embeddings for assay similarity\")\n",
    "        else:\n",
    "            self.embeddings = None\n",
    "            logger.warning(\"âš ï¸ LangChain not available, assay similarity disabled\")\n",
    "        \n",
    "        # RDKit ì»´í¬ë„ŒíŠ¸\n",
    "        self.mol_objects = {}\n",
    "        self.fingerprints = {}\n",
    "        \n",
    "        # ë¹„ìš© ì¶”ì \n",
    "        self.cost_tracker = {\n",
    "            'input_tokens': 0,\n",
    "            'output_tokens': 0,\n",
    "            'total_cost': 0.0,\n",
    "            'api_calls': 0\n",
    "        }\n",
    "        \n",
    "        logger.info(\"ğŸ”¬ Hybrid RAG System initialized\")\n",
    "    \n",
    "    def load_jsonl_data(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"JSONL íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "        data = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line.strip())\n",
    "                    data.append(item)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "        \n",
    "        logger.info(f\"Loaded {len(data)} samples from {file_path}\")\n",
    "        return data\n",
    "    \n",
    "    def simple_train_test_split(self, data: List[Dict], test_size: float = 0.2, random_state: int = 42) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"ê°„ë‹¨í•œ train/test ë¶„í• \"\"\"\n",
    "        random.seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        shuffled_data = data.copy()\n",
    "        random.shuffle(shuffled_data)\n",
    "        \n",
    "        split_idx = int(len(shuffled_data) * (1 - test_size))\n",
    "        train_data = shuffled_data[:split_idx]\n",
    "        test_data = shuffled_data[split_idx:]\n",
    "        \n",
    "        logger.info(f\"Train set: {len(train_data)} samples\")\n",
    "        logger.info(f\"Test set: {len(test_data)} samples\")\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    def prepare_hybrid_training_data(self, train_data: List[Dict]):\n",
    "        \"\"\"í•˜ì´ë¸Œë¦¬ë“œ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„\"\"\"\n",
    "        logger.info(\"ğŸ—ï¸ Preparing hybrid training data...\")\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.parsed_train_data = []\n",
    "        assay_documents = []\n",
    "        \n",
    "        for idx, item in enumerate(train_data):\n",
    "            # 1. í…ìŠ¤íŠ¸ íŒŒì‹±\n",
    "            parsed = DataParser.parse_input_text(item['input_text'])\n",
    "            parsed['logac50'] = int(item['output_text'])\n",
    "            parsed['idx'] = idx\n",
    "            \n",
    "            # 2. í™”í•™ êµ¬ì¡° ì²˜ë¦¬ (RDKit)\n",
    "            if RDKIT_AVAILABLE and parsed['smiles']:\n",
    "                mol = self._create_mol_object(parsed['smiles'])\n",
    "                if mol is not None:\n",
    "                    parsed['mol'] = mol\n",
    "                    parsed['fingerprints'] = self._generate_fingerprints(mol, parsed['smiles'])\n",
    "                    parsed['molecular_props'] = self._calculate_molecular_properties(mol)\n",
    "                    parsed['activity_category'] = self._categorize_activity(parsed['logac50'])\n",
    "                    \n",
    "                    # ìºì‹±\n",
    "                    self.mol_objects[parsed['smiles']] = mol\n",
    "                    self.fingerprints[parsed['smiles']] = parsed['fingerprints']\n",
    "            \n",
    "            # 3. ì‹¤í—˜ ì¡°ê±´ ë¬¸ì„œ ìƒì„± (LangChainìš©)\n",
    "            if LANGCHAIN_AVAILABLE and parsed['assay_description']:\n",
    "                assay_doc_content = f\"\"\"\n",
    "                Assay: {parsed['assay_name']}\n",
    "                Description: {parsed['assay_description']}\n",
    "                Activity: {parsed['logac50']}\n",
    "                Category: {self._categorize_activity(parsed['logac50'])}\n",
    "                Instructions: {parsed['instruction']}\n",
    "                \"\"\"\n",
    "                \n",
    "                assay_doc = Document(\n",
    "                    page_content=assay_doc_content,\n",
    "                    metadata={\n",
    "                        'assay_name': parsed['assay_name'],\n",
    "                        'logac50': parsed['logac50'],\n",
    "                        'idx': idx\n",
    "                    }\n",
    "                )\n",
    "                assay_documents.append(assay_doc)\n",
    "            \n",
    "            self.parsed_train_data.append(parsed)\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                logger.info(f\"Processed {idx + 1}/{len(train_data)} samples...\")\n",
    "        \n",
    "        # 4. LangChain ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•\n",
    "        if LANGCHAIN_AVAILABLE and assay_documents:\n",
    "            logger.info(\"ğŸ”¤ Building assay vector store...\")\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=50\n",
    "            )\n",
    "            split_docs = text_splitter.split_documents(assay_documents)\n",
    "            self.assay_vectorstore = FAISS.from_documents(split_docs, self.embeddings)\n",
    "            logger.info(f\"âœ… Built assay vector store with {len(split_docs)} chunks\")\n",
    "        \n",
    "        logger.info(f\"âœ… Prepared {len(self.parsed_train_data)} training examples\")\n",
    "    \n",
    "    def _categorize_activity(self, logac50: int) -> str:\n",
    "        \"\"\"í™œì„±ë„ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜\"\"\"\n",
    "        if logac50 >= 85:\n",
    "            return \"Very High\"\n",
    "        elif logac50 >= 70:\n",
    "            return \"High\"\n",
    "        elif logac50 >= 50:\n",
    "            return \"Medium\"\n",
    "        elif logac50 >= 30:\n",
    "            return \"Low\"\n",
    "        else:\n",
    "            return \"Very Low\"\n",
    "    \n",
    "    def _create_mol_object(self, smiles: str):\n",
    "        \"\"\"SMILESì—ì„œ RDKit ë¶„ì ê°ì²´ ìƒì„±\"\"\"\n",
    "        if not RDKIT_AVAILABLE:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                mol = Chem.AddHs(mol)\n",
    "                return mol\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error creating mol object for {smiles}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _generate_fingerprints(self, mol, smiles: str) -> Dict:\n",
    "        \"\"\"ë‹¤ì–‘í•œ ë¶„ì ì§€ë¬¸ ìƒì„±\"\"\"\n",
    "        if not RDKIT_AVAILABLE or mol is None:\n",
    "            return {}\n",
    "        \n",
    "        fingerprints = {}\n",
    "        try:\n",
    "            fingerprints['morgan'] = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "            fingerprints['maccs'] = MACCSkeys.GenMACCSKeys(mol)\n",
    "            fingerprints['rdkit'] = AllChem.GetRDKitFPGenerator().GetFingerprint(mol)\n",
    "            fingerprints['atompair'] = AllChem.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=2048)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error generating fingerprints for {smiles}: {e}\")\n",
    "        \n",
    "        return fingerprints\n",
    "    \n",
    "    def _calculate_molecular_properties(self, mol) -> Dict:\n",
    "        \"\"\"ë¶„ì ë¬¼ì„± ê³„ì‚°\"\"\"\n",
    "        if not RDKIT_AVAILABLE or mol is None:\n",
    "            return {}\n",
    "            \n",
    "        try:\n",
    "            props = {\n",
    "                'mw': Descriptors.MolWt(mol),\n",
    "                'logp': Descriptors.MolLogP(mol),\n",
    "                'hbd': Descriptors.NumHDonors(mol),\n",
    "                'hba': Descriptors.NumHAcceptors(mol),\n",
    "                'tpsa': Descriptors.TPSA(mol),\n",
    "                'rotatable_bonds': Descriptors.NumRotatableBonds(mol),\n",
    "                'aromatic_rings': Descriptors.NumAromaticRings(mol),\n",
    "                'heavy_atoms': Descriptors.HeavyAtomCount(mol)\n",
    "            }\n",
    "            return props\n",
    "        except Exception:\n",
    "            return {}\n",
    "    \n",
    "    def hybrid_similarity_search(self, query_input: str, k_assay: int = 3, k_chemical: int = 5) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ìœ ì‚¬ë„ ê²€ìƒ‰: ì‹¤í—˜ì¡°ê±´ + í™”í•™êµ¬ì¡°\"\"\"\n",
    "        \n",
    "        # 1. ì…ë ¥ íŒŒì‹±\n",
    "        parsed_query = DataParser.parse_input_text(query_input)\n",
    "        \n",
    "        # 2. ì‹¤í—˜ ì¡°ê±´ ìœ ì‚¬ë„ ê²€ìƒ‰ (LangChain)\n",
    "        similar_assays = []\n",
    "        if LANGCHAIN_AVAILABLE and self.assay_vectorstore and parsed_query['assay_description']:\n",
    "            try:\n",
    "                assay_query = f\"{parsed_query['assay_name']} {parsed_query['assay_description']}\"\n",
    "                assay_docs = self.assay_vectorstore.similarity_search_with_score(\n",
    "                    assay_query, k=k_assay\n",
    "                )\n",
    "                \n",
    "                for doc, score in assay_docs:\n",
    "                    similar_assays.append({\n",
    "                        'content': doc.page_content,\n",
    "                        'metadata': doc.metadata,\n",
    "                        'similarity_score': 1 - score,  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜\n",
    "                        'assay_name': doc.metadata.get('assay_name', ''),\n",
    "                        'logac50': doc.metadata.get('logac50', 0)\n",
    "                    })\n",
    "                    \n",
    "                logger.debug(f\"Found {len(similar_assays)} similar assays\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Assay search failed: {e}\")\n",
    "        \n",
    "        # 3. í™”í•™ì  ìœ ì‚¬ë„ ê²€ìƒ‰ (RDKit)\n",
    "        similar_molecules = []\n",
    "        if RDKIT_AVAILABLE and parsed_query['smiles']:\n",
    "            try:\n",
    "                query_mol = self._create_mol_object(parsed_query['smiles'])\n",
    "                if query_mol is not None:\n",
    "                    query_fps = self._generate_fingerprints(query_mol, parsed_query['smiles'])\n",
    "                    \n",
    "                    similarities = []\n",
    "                    for example in self.parsed_train_data:\n",
    "                        if 'fingerprints' in example and example['fingerprints']:\n",
    "                            similarity_scores = self._calculate_multi_fingerprint_similarity(\n",
    "                                query_fps, example['fingerprints']\n",
    "                            )\n",
    "                            final_similarity = self._combine_similarity_scores(similarity_scores)\n",
    "                            \n",
    "                            similarities.append((final_similarity, example, similarity_scores))\n",
    "                    \n",
    "                    # ìƒìœ„ kê°œ ì„ íƒ\n",
    "                    similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "                    \n",
    "                    for sim_score, example, breakdown in similarities[:k_chemical]:\n",
    "                        similar_molecules.append({\n",
    "                            'smiles': example['smiles'],\n",
    "                            'logac50': example['logac50'],\n",
    "                            'activity_category': example.get('activity_category', ''),\n",
    "                            'molecular_props': example.get('molecular_props', {}),\n",
    "                            'similarity_score': sim_score,\n",
    "                            'similarity_breakdown': breakdown,\n",
    "                            'assay_name': example.get('assay_name', '')\n",
    "                        })\n",
    "                    \n",
    "                    logger.debug(f\"Found {len(similar_molecules)} similar molecules\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Chemical search failed: {e}\")\n",
    "        \n",
    "        return similar_assays, similar_molecules\n",
    "    \n",
    "    def _calculate_multi_fingerprint_similarity(self, query_fps: Dict, target_fps: Dict) -> Dict:\n",
    "        \"\"\"ë‹¤ì¤‘ ì§€ë¬¸ì„ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "        similarities = {}\n",
    "        \n",
    "        fingerprint_types = ['morgan', 'maccs', 'rdkit', 'atompair']\n",
    "        \n",
    "        for fp_type in fingerprint_types:\n",
    "            if fp_type in query_fps and fp_type in target_fps:\n",
    "                try:\n",
    "                    tanimoto_sim = DataStructs.TanimotoSimilarity(\n",
    "                        query_fps[fp_type], target_fps[fp_type]\n",
    "                    )\n",
    "                    similarities[fp_type] = tanimoto_sim\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error calculating {fp_type} similarity: {e}\")\n",
    "                    similarities[fp_type] = 0.0\n",
    "            else:\n",
    "                similarities[fp_type] = 0.0\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def _combine_similarity_scores(self, similarity_scores: Dict) -> float:\n",
    "        \"\"\"ì—¬ëŸ¬ ì§€ë¬¸ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê²°í•©\"\"\"\n",
    "        weights = {\n",
    "            'morgan': 0.4,\n",
    "            'maccs': 0.3,\n",
    "            'rdkit': 0.2,\n",
    "            'atompair': 0.1\n",
    "        }\n",
    "        \n",
    "        weighted_sum = 0.0\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        for fp_type, weight in weights.items():\n",
    "            if fp_type in similarity_scores:\n",
    "                weighted_sum += similarity_scores[fp_type] * weight\n",
    "                total_weight += weight\n",
    "        \n",
    "        return weighted_sum / total_weight if total_weight > 0 else 0.0\n",
    "    \n",
    "    def calculate_context_weights(self, similar_assays: List[Dict], similar_molecules: List[Dict]) -> Dict:\n",
    "        \"\"\"ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ë™ì  ê³„ì‚°\"\"\"\n",
    "        \n",
    "        # ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ì¶œ\n",
    "        max_assay_sim = max([assay.get('similarity_score', 0) for assay in similar_assays]) if similar_assays else 0\n",
    "        max_chem_sim = max([mol.get('similarity_score', 0) for mol in similar_molecules]) if similar_molecules else 0\n",
    "        \n",
    "        # ë°ì´í„° ê°€ìš©ì„± ê³ ë ¤\n",
    "        assay_availability = len(similar_assays) / 3.0  # ìµœëŒ€ 3ê°œ ëŒ€ë¹„\n",
    "        chem_availability = len(similar_molecules) / 5.0  # ìµœëŒ€ 5ê°œ ëŒ€ë¹„\n",
    "        \n",
    "        # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "        if max_assay_sim > 0.8 and max_chem_sim < 0.5:\n",
    "            # ì‹¤í—˜ ì¡°ê±´ ë§¤ìš° ìœ ì‚¬, í™”í•™ êµ¬ì¡° ë‹¤ë¦„\n",
    "            weights = {'assay': 0.75, 'chemical': 0.25}\n",
    "        elif max_chem_sim > 0.8 and max_assay_sim < 0.5:\n",
    "            # í™”í•™ êµ¬ì¡° ë§¤ìš° ìœ ì‚¬, ì‹¤í—˜ ì¡°ê±´ ë‹¤ë¦„\n",
    "            weights = {'assay': 0.25, 'chemical': 0.75}\n",
    "        elif max_assay_sim > 0.7 and max_chem_sim > 0.7:\n",
    "            # ë‘˜ ë‹¤ ìœ ì‚¬í•¨ - ê· í˜•\n",
    "            weights = {'assay': 0.5, 'chemical': 0.5}\n",
    "        else:\n",
    "            # ê¸°ë³¸ ê°€ì¤‘ì¹˜, ê°€ìš©ì„±ìœ¼ë¡œ ì¡°ì •\n",
    "            base_assay_weight = 0.4 + (assay_availability * 0.2)\n",
    "            base_chem_weight = 0.6 - (assay_availability * 0.2)\n",
    "            weights = {'assay': base_assay_weight, 'chemical': base_chem_weight}\n",
    "        \n",
    "        # ì •ê·œí™”\n",
    "        total = weights['assay'] + weights['chemical']\n",
    "        weights = {k: v/total for k, v in weights.items()}\n",
    "        \n",
    "        logger.debug(f\"Context weights: Assay={weights['assay']:.2f}, Chemical={weights['chemical']:.2f}\")\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def create_hybrid_prompt(self, query_input: str, similar_assays: List[Dict], similar_molecules: List[Dict], weights: Dict) -> str:\n",
    "        \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸ í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
    "        \n",
    "        parsed_query = DataParser.parse_input_text(query_input)\n",
    "        \n",
    "        # ì‹¤í—˜ ì¡°ê±´ ì»¨í…ìŠ¤íŠ¸\n",
    "        assay_context = \"\"\n",
    "        if similar_assays:\n",
    "            assay_context = f\"ğŸ§ª EXPERIMENTAL PROTOCOL CONTEXT (Weight: {weights['assay']:.2f}):\\n\\n\"\n",
    "            for i, assay in enumerate(similar_assays, 1):\n",
    "                assay_context += f\"Similar Assay {i} (Similarity: {assay['similarity_score']:.3f}):\\n\"\n",
    "                assay_context += f\"  {assay['content']}\\n\\n\"\n",
    "        else:\n",
    "            assay_context = \"ğŸ§ª EXPERIMENTAL PROTOCOL CONTEXT: No similar assays found.\\n\\n\"\n",
    "        \n",
    "        # í™”í•™ êµ¬ì¡° ì»¨í…ìŠ¤íŠ¸\n",
    "        chemical_context = \"\"\n",
    "        if similar_molecules:\n",
    "            chemical_context = f\"ğŸ§¬ CHEMICAL STRUCTURE CONTEXT (Weight: {weights['chemical']:.2f}):\\n\\n\"\n",
    "            for i, mol in enumerate(similar_molecules, 1):\n",
    "                chemical_context += f\"Similar Molecule {i} (Tanimoto: {mol['similarity_score']:.3f}):\\n\"\n",
    "                chemical_context += f\"  SMILES: {mol['smiles']}\\n\"\n",
    "                chemical_context += f\"  LogAC50: {mol['logac50']}\\n\"\n",
    "                chemical_context += f\"  Activity: {mol['activity_category']}\\n\"\n",
    "                \n",
    "                if 'similarity_breakdown' in mol:\n",
    "                    breakdown = mol['similarity_breakdown']\n",
    "                    chemical_context += f\"  Fingerprint Details:\\n\"\n",
    "                    chemical_context += f\"    - Morgan: {breakdown.get('morgan', 0):.3f}\\n\"\n",
    "                    chemical_context += f\"    - MACCS: {breakdown.get('maccs', 0):.3f}\\n\"\n",
    "                    chemical_context += f\"    - RDKit: {breakdown.get('rdkit', 0):.3f}\\n\"\n",
    "                \n",
    "                if 'molecular_props' in mol and mol['molecular_props']:\n",
    "                    props = mol['molecular_props']\n",
    "                    chemical_context += f\"  Properties: MW={props.get('mw', 'N/A'):.1f}, \"\n",
    "                    chemical_context += f\"LogP={props.get('logp', 'N/A'):.2f}\\n\"\n",
    "                \n",
    "                chemical_context += \"\\n\"\n",
    "        else:\n",
    "            chemical_context = \"ğŸ§¬ CHEMICAL STRUCTURE CONTEXT: No similar molecules found.\\n\\n\"\n",
    "        \n",
    "        # í†µí•© í”„ë¡¬í”„íŠ¸\n",
    "        integrated_prompt = f\"\"\"<thinking>\n",
    "I am performing a hybrid analysis combining experimental protocol knowledge and chemical structure similarity for toxicity prediction.\n",
    "\n",
    "Query Details:\n",
    "- Assay: {parsed_query['assay_name']}\n",
    "- SMILES: {parsed_query['smiles']}\n",
    "\n",
    "Context Analysis:\n",
    "- Assay context weight: {weights['assay']:.2f}\n",
    "- Chemical context weight: {weights['chemical']:.2f}\n",
    "\n",
    "This weighting suggests I should prioritize {\"experimental context\" if weights['assay'] > weights['chemical'] else \"chemical structure analysis\"} while considering both sources of information.\n",
    "\n",
    "Let me analyze the patterns systematically...\n",
    "</thinking>\n",
    "\n",
    "{assay_context}\n",
    "\n",
    "{chemical_context}\n",
    "\n",
    "ğŸ¯ HYBRID TOXICITY PREDICTION TASK:\n",
    "\n",
    "Query Input:\n",
    "- Assay: {parsed_query['assay_name']}\n",
    "- SMILES: {parsed_query['smiles']}\n",
    "- Task: {parsed_query['instruction']}\n",
    "\n",
    "ğŸ“Š INTEGRATED ANALYSIS FRAMEWORK:\n",
    "\n",
    "1. **Context Weighting Strategy**:\n",
    "   - Experimental Protocol Weight: {weights['assay']:.2f}\n",
    "   - Chemical Structure Weight: {weights['chemical']:.2f}\n",
    "\n",
    "2. **Primary Analysis Focus**:\n",
    "   {\"Focus on experimental protocol patterns and assay-specific factors\" if weights['assay'] > 0.6 else \"Focus on chemical structure-activity relationships\" if weights['chemical'] > 0.6 else \"Balance both experimental and chemical contexts equally\"}\n",
    "\n",
    "3. **Cross-Validation Approach**:\n",
    "   - Compare patterns from both experimental and chemical contexts\n",
    "   - Identify consistent vs conflicting predictions\n",
    "   - Resolve conflicts using the higher-weighted context\n",
    "\n",
    "4. **Evidence Integration**:\n",
    "   - Experimental evidence: {\"Strong\" if len(similar_assays) >= 2 else \"Moderate\" if len(similar_assays) == 1 else \"Weak\"}\n",
    "   - Chemical evidence: {\"Strong\" if len(similar_molecules) >= 3 else \"Moderate\" if len(similar_molecules) >= 1 else \"Weak\"}\n",
    "\n",
    "ğŸ”¬ REQUIRED ANALYSIS:\n",
    "\n",
    "**EXPERIMENTAL CONTEXT ANALYSIS**:\n",
    "[Analyze the experimental protocol patterns and assay-specific factors]\n",
    "\n",
    "**CHEMICAL STRUCTURE ANALYSIS**:\n",
    "[Analyze the molecular structure and chemical similarity patterns]\n",
    "\n",
    "**INTEGRATED PREDICTION LOGIC**:\n",
    "[Combine both contexts using the calculated weights]\n",
    "\n",
    "**FINAL PREDICTION**: [INTEGER 0-100]\n",
    "\n",
    "**CONFIDENCE ASSESSMENT**: [High/Medium/Low with justification based on context quality]\n",
    "\n",
    "Remember: Weight your analysis according to the calculated context weights, but always provide reasoning from both experimental and chemical perspectives when available.\"\"\"\n",
    "\n",
    "        return integrated_prompt\n",
    "    \n",
    "    def predict_single(self, query_input: str) -> Tuple[int, str, float, Dict]:\n",
    "        \"\"\"ë‹¨ì¼ ì…ë ¥ì— ëŒ€í•œ í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 1. í•˜ì´ë¸Œë¦¬ë“œ ìœ ì‚¬ë„ ê²€ìƒ‰\n",
    "            similar_assays, similar_molecules = self.hybrid_similarity_search(query_input)\n",
    "            \n",
    "            # 2. ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "            weights = self.calculate_context_weights(similar_assays, similar_molecules)\n",
    "            \n",
    "            # 3. í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "            prompt = self.create_hybrid_prompt(query_input, similar_assays, similar_molecules, weights)\n",
    "            \n",
    "            # 4. Claude API í˜¸ì¶œ\n",
    "            response = self.claude_client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=3000,  # í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ì„ ìœ„í•œ ë” ê¸´ ì‘ë‹µ\n",
    "                temperature=self.temperature,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            result_text = response.content[0].text\n",
    "            \n",
    "            # 5. í† í° ì‚¬ìš©ëŸ‰ ì¶”ì \n",
    "            self.cost_tracker['input_tokens'] += response.usage.input_tokens\n",
    "            self.cost_tracker['output_tokens'] += response.usage.output_tokens\n",
    "            self.cost_tracker['api_calls'] += 1\n",
    "            \n",
    "            # 6. ì˜ˆì¸¡ê°’ ì¶”ì¶œ\n",
    "            prediction = self._extract_prediction(result_text)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            # 7. ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘\n",
    "            metadata = {\n",
    "                'weights': weights,\n",
    "                'n_similar_assays': len(similar_assays),\n",
    "                'n_similar_molecules': len(similar_molecules),\n",
    "                'max_assay_similarity': max([a.get('similarity_score', 0) for a in similar_assays]) if similar_assays else 0,\n",
    "                'max_chemical_similarity': max([m.get('similarity_score', 0) for m in similar_molecules]) if similar_molecules else 0\n",
    "            }\n",
    "            \n",
    "            logger.debug(f\"Hybrid prediction: {prediction} (assay_weight: {weights['assay']:.2f}, chem_weight: {weights['chemical']:.2f})\")\n",
    "            \n",
    "            return prediction, result_text, elapsed_time, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in hybrid prediction: {e}\")\n",
    "            return 50, f\"Error: {str(e)}\", 0.0, {}\n",
    "    \n",
    "    def _extract_prediction(self, result_text: str) -> int:\n",
    "        \"\"\"LLM ì‘ë‹µì—ì„œ ì˜ˆì¸¡ê°’ ì¶”ì¶œ\"\"\"\n",
    "        patterns = [\n",
    "            r'FINAL PREDICTION:\\s*(\\d{1,3})',\n",
    "            r'PREDICTION:\\s*(\\d{1,3})',\n",
    "            r'Prediction:\\s*(\\d{1,3})',\n",
    "            r'LogAC50:\\s*(\\d{1,3})',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, result_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                val = int(match.group(1))\n",
    "                if 0 <= val <= 100:\n",
    "                    return val\n",
    "        \n",
    "        # ë°±ì—…: 0-100 ë²”ìœ„ì˜ ì²« ë²ˆì§¸ ìˆ«ì\n",
    "        numbers = re.findall(r'\\b(\\d{1,3})\\b', result_text)\n",
    "        for num in numbers:\n",
    "            val = int(num)\n",
    "            if 0 <= val <= 100:\n",
    "                return val\n",
    "        \n",
    "        logger.warning(\"Could not extract valid prediction, using default value 50\")\n",
    "        return 50\n",
    "    \n",
    "    def evaluate_test_set(self, test_data: List[Dict]) -> Dict:\n",
    "        \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\"\"\"\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        explanations = []\n",
    "        times = []\n",
    "        errors = []\n",
    "        metadata_list = []\n",
    "        \n",
    "        logger.info(f\"ğŸ”¬ Starting hybrid evaluation on {len(test_data)} test samples...\")\n",
    "        \n",
    "        for i, item in enumerate(test_data):\n",
    "            input_text = item['input_text']\n",
    "            actual = int(item['output_text'])\n",
    "            \n",
    "            try:\n",
    "                pred, explanation, elapsed_time, metadata = self.predict_single(input_text)\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                actuals.append(actual)\n",
    "                explanations.append(explanation)\n",
    "                times.append(elapsed_time)\n",
    "                errors.append(None)\n",
    "                metadata_list.append(metadata)\n",
    "                \n",
    "                # ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\n",
    "                if i > 0:\n",
    "                    current_mae = np.mean([abs(a - p) for a, p in zip(actuals, predictions)])\n",
    "                    logger.info(f\"Sample {i+1}/{len(test_data)} | Pred: {pred} | Actual: {actual} | \"\n",
    "                              f\"Weights: A={metadata['weights']['assay']:.2f}/C={metadata['weights']['chemical']:.2f} | \"\n",
    "                              f\"Running MAE: {current_mae:.2f}\")\n",
    "                else:\n",
    "                    logger.info(f\"Sample {i+1}/{len(test_data)} | Pred: {pred} | Actual: {actual}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing sample {i}: {e}\")\n",
    "                predictions.append(50)\n",
    "                actuals.append(actual)\n",
    "                explanations.append(f\"Error: {str(e)}\")\n",
    "                times.append(0.0)\n",
    "                errors.append(str(e))\n",
    "                metadata_list.append({})\n",
    "        \n",
    "        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        mae = np.mean([abs(a - p) for a, p in zip(actuals, predictions)])\n",
    "        mse = np.mean([(a - p)**2 for a, p in zip(actuals, predictions)])\n",
    "        rmse = math.sqrt(mse)\n",
    "        r2 = self._calculate_r2(actuals, predictions)\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ íŠ¹í™” ë©”íŠ¸ë¦­\n",
    "        assay_weighted_samples = [i for i, meta in enumerate(metadata_list) \n",
    "                                 if meta.get('weights', {}).get('assay', 0) > 0.6]\n",
    "        chemical_weighted_samples = [i for i, meta in enumerate(metadata_list) \n",
    "                                   if meta.get('weights', {}).get('chemical', 0) > 0.6]\n",
    "        balanced_samples = [i for i, meta in enumerate(metadata_list) \n",
    "                           if 0.4 <= meta.get('weights', {}).get('assay', 0.5) <= 0.6]\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ë³„ ì„±ëŠ¥ ë¶„ì„\n",
    "        assay_mae = np.mean([abs(actuals[i] - predictions[i]) for i in assay_weighted_samples]) if assay_weighted_samples else float('inf')\n",
    "        chemical_mae = np.mean([abs(actuals[i] - predictions[i]) for i in chemical_weighted_samples]) if chemical_weighted_samples else float('inf')\n",
    "        balanced_mae = np.mean([abs(actuals[i] - predictions[i]) for i in balanced_samples]) if balanced_samples else float('inf')\n",
    "        \n",
    "        # ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ì„\n",
    "        high_assay_sim_samples = [i for i, meta in enumerate(metadata_list) \n",
    "                                 if meta.get('max_assay_similarity', 0) > 0.7]\n",
    "        high_chem_sim_samples = [i for i, meta in enumerate(metadata_list) \n",
    "                                if meta.get('max_chemical_similarity', 0) > 0.7]\n",
    "        \n",
    "        results = {\n",
    "            'predictions': predictions,\n",
    "            'actuals': actuals,\n",
    "            'explanations': explanations,\n",
    "            'times': times,\n",
    "            'errors': errors,\n",
    "            'metadata': metadata_list,\n",
    "            'metrics': {\n",
    "                'mae': mae,\n",
    "                'mse': mse,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'within_10_pct': sum([1 for a, p in zip(actuals, predictions) if abs(a - p) <= 10]) / len(actuals) * 100,\n",
    "                'within_20_pct': sum([1 for a, p in zip(actuals, predictions) if abs(a - p) <= 20]) / len(actuals) * 100,\n",
    "                'assay_weighted_mae': assay_mae,\n",
    "                'chemical_weighted_mae': chemical_mae,\n",
    "                'balanced_mae': balanced_mae,\n",
    "                'n_assay_weighted': len(assay_weighted_samples),\n",
    "                'n_chemical_weighted': len(chemical_weighted_samples),\n",
    "                'n_balanced': len(balanced_samples),\n",
    "                'n_high_assay_sim': len(high_assay_sim_samples),\n",
    "                'n_high_chem_sim': len(high_chem_sim_samples),\n",
    "                'avg_assay_weight': np.mean([meta.get('weights', {}).get('assay', 0.5) for meta in metadata_list]),\n",
    "                'avg_chemical_weight': np.mean([meta.get('weights', {}).get('chemical', 0.5) for meta in metadata_list]),\n",
    "                'n_samples': len(test_data),\n",
    "                'n_errors': len([e for e in errors if e is not None]),\n",
    "                'avg_time': np.mean([t for t in times if t > 0])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_r2(self, y_true: List[float], y_pred: List[float]) -> float:\n",
    "        \"\"\"RÂ² Score ê³„ì‚°\"\"\"\n",
    "        y_mean = np.mean(y_true)\n",
    "        ss_tot = sum([(y - y_mean)**2 for y in y_true])\n",
    "        ss_res = sum([(t - p)**2 for t, p in zip(y_true, y_pred)])\n",
    "        return 1 - (ss_res / ss_tot) if ss_tot != 0 else 0.0\n",
    "    \n",
    "    def calculate_cost(self) -> Dict:\n",
    "        \"\"\"Claude API ë¹„ìš© ê³„ì‚°\"\"\"\n",
    "        input_cost_per_1k = 0.003   # $3/1M = $0.003/1K\n",
    "        output_cost_per_1k = 0.015  # $15/1M = $0.015/1K\n",
    "        \n",
    "        input_cost = (self.cost_tracker['input_tokens'] / 1000) * input_cost_per_1k\n",
    "        output_cost = (self.cost_tracker['output_tokens'] / 1000) * output_cost_per_1k\n",
    "        total_cost = input_cost + output_cost\n",
    "        \n",
    "        return {\n",
    "            'input_tokens': self.cost_tracker['input_tokens'],\n",
    "            'output_tokens': self.cost_tracker['output_tokens'],\n",
    "            'api_calls': self.cost_tracker['api_calls'],\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': total_cost\n",
    "        }\n",
    "    \n",
    "    def print_results(self, results: Dict, cost_breakdown: Dict):\n",
    "        \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ê²°ê³¼ ì¶œë ¥\"\"\"\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"ğŸ”¬ HYBRID RAG SYSTEM - SMILES TOXICITY PREDICTION RESULTS\")\n",
    "        print(\"=\"*90)\n",
    "        \n",
    "        metrics = results['metrics']\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Overall Performance Metrics:\")\n",
    "        print(f\"   â€¢ Mean Absolute Error (MAE): {metrics['mae']:.2f}\")\n",
    "        print(f\"   â€¢ Root Mean Square Error (RMSE): {metrics['rmse']:.2f}\")\n",
    "        print(f\"   â€¢ RÂ² Score: {metrics['r2']:.3f}\")\n",
    "        print(f\"   â€¢ Accuracy within Â±10: {metrics['within_10_pct']:.1f}%\")\n",
    "        print(f\"   â€¢ Accuracy within Â±20: {metrics['within_20_pct']:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nğŸ”€ Hybrid Analysis Breakdown:\")\n",
    "        print(f\"   â€¢ Average Assay Weight: {metrics['avg_assay_weight']:.2f}\")\n",
    "        print(f\"   â€¢ Average Chemical Weight: {metrics['avg_chemical_weight']:.2f}\")\n",
    "        print(f\"   â€¢ Assay-Weighted Samples: {metrics['n_assay_weighted']} (MAE: {metrics['assay_weighted_mae']:.2f})\")\n",
    "        print(f\"   â€¢ Chemical-Weighted Samples: {metrics['n_chemical_weighted']} (MAE: {metrics['chemical_weighted_mae']:.2f})\")\n",
    "        print(f\"   â€¢ Balanced Samples: {metrics['n_balanced']} (MAE: {metrics['balanced_mae']:.2f})\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Similarity Analysis:\")\n",
    "        print(f\"   â€¢ High Assay Similarity (>0.7): {metrics['n_high_assay_sim']} samples\")\n",
    "        print(f\"   â€¢ High Chemical Similarity (>0.7): {metrics['n_high_chem_sim']} samples\")\n",
    "        \n",
    "        print(f\"\\nâš¡ Performance:\")\n",
    "        print(f\"   â€¢ Average Prediction Time: {metrics['avg_time']:.2f}s\")\n",
    "        print(f\"   â€¢ Test Samples: {metrics['n_samples']}\")\n",
    "        print(f\"   â€¢ Errors: {metrics['n_errors']}\")\n",
    "        \n",
    "        print(f\"\\nğŸ’° Cost Breakdown (Claude Sonnet 4):\")\n",
    "        print(f\"   â€¢ API Calls: {cost_breakdown['api_calls']}\")\n",
    "        print(f\"   â€¢ Input Tokens: {cost_breakdown['input_tokens']:,}\")\n",
    "        print(f\"   â€¢ Output Tokens: {cost_breakdown['output_tokens']:,}\")\n",
    "        print(f\"   â€¢ Total Cost: ${cost_breakdown['total_cost']:.4f}\")\n",
    "        \n",
    "        # ìµœê³ /ìµœì•… ì˜ˆì¸¡ ë¶„ì„\n",
    "        errors = [abs(a - p) for a, p in zip(results['actuals'], results['predictions'])]\n",
    "        best_indices = np.argsort(errors)[:3]\n",
    "        worst_indices = np.argsort(errors)[-3:][::-1]\n",
    "        \n",
    "        print(f\"\\nğŸ† Best Hybrid Predictions:\")\n",
    "        for i, idx in enumerate(best_indices):\n",
    "            actual = results['actuals'][idx]\n",
    "            pred = results['predictions'][idx]\n",
    "            metadata = results['metadata'][idx]\n",
    "            error = abs(actual - pred)\n",
    "            weights = metadata.get('weights', {})\n",
    "            print(f\"   {i+1}. Actual: {actual:2d}, Predicted: {pred:2d}, Error: {error:2d}, \"\n",
    "                  f\"Weights: A={weights.get('assay', 0):.2f}/C={weights.get('chemical', 0):.2f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¤” Most Challenging Predictions:\")\n",
    "        for i, idx in enumerate(worst_indices):\n",
    "            actual = results['actuals'][idx]\n",
    "            pred = results['predictions'][idx]\n",
    "            metadata = results['metadata'][idx]\n",
    "            error = abs(actual - pred)\n",
    "            weights = metadata.get('weights', {})\n",
    "            print(f\"   {i+1}. Actual: {actual:2d}, Predicted: {pred:2d}, Error: {error:2d}, \"\n",
    "                  f\"Weights: A={weights.get('assay', 0):.2f}/C={weights.get('chemical', 0):.2f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    DATA_FILE = \"combined_train_sampled2.jsonl\"\n",
    "    TEST_SIZE = 0.2\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # API í‚¤ ì„¤ì •\n",
    "    CLAUDE_API_KEY = \"\"  # ì‹¤ì œ í‚¤ë¡œ êµì²´í•˜ì„¸ìš”\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ”¬ Initializing Hybrid RAG System...\")\n",
    "        print(\"   â€¢ Natural Language Processing: LangChain + Local Embeddings\")\n",
    "        print(\"   â€¢ Chemical Similarity: RDKit + Molecular Fingerprints\")\n",
    "        print(\"   â€¢ Integration: Claude Sonnet 4\")\n",
    "        \n",
    "        predictor = HybridSMILESRAG(claude_api_key=CLAUDE_API_KEY)\n",
    "        \n",
    "        print(\"\\nğŸ“š Loading data...\")\n",
    "        data = predictor.load_jsonl_data(DATA_FILE)\n",
    "        \n",
    "        print(\"âœ‚ï¸ Splitting data...\")\n",
    "        train_data, test_data = predictor.simple_train_test_split(\n",
    "            data, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        print(\"ğŸ—ï¸ Preparing hybrid training data...\")\n",
    "        print(\"   â€¢ Parsing experimental conditions and SMILES\")\n",
    "        print(\"   â€¢ Building assay vector store (LangChain)\")\n",
    "        print(\"   â€¢ Generating molecular fingerprints (RDKit)\")\n",
    "        predictor.prepare_hybrid_training_data(train_data)\n",
    "        \n",
    "        print(\"\\nğŸ”¬ Evaluating with Hybrid RAG System...\")\n",
    "        print(\"   â€¢ Experimental similarity search + Chemical similarity search\")\n",
    "        print(\"   â€¢ Dynamic context weighting + Integrated reasoning\")\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "        test_subset = test_data[:20]  # ë¹„ìš© ê³ ë ¤í•˜ì—¬ 20ê°œë¡œ ì‹œì‘\n",
    "        results = predictor.evaluate_test_set(test_subset)\n",
    "        \n",
    "        print(\"\\nğŸ’° Calculating costs...\")\n",
    "        cost_breakdown = predictor.calculate_cost()\n",
    "        \n",
    "        print(\"\\nğŸ“Š Generating comprehensive results...\")\n",
    "        predictor.print_results(results, cost_breakdown)\n",
    "        \n",
    "        # ìƒì„¸ ê²°ê³¼ ì €ì¥\n",
    "        output_data = {\n",
    "            'model': 'hybrid-rag-claude-sonnet-4',\n",
    "            'components': {\n",
    "                'assay_similarity': 'LangChain + HuggingFace Embeddings' if LANGCHAIN_AVAILABLE else 'Disabled',\n",
    "                'chemical_similarity': 'RDKit + Molecular Fingerprints' if RDKIT_AVAILABLE else 'String Fallback',\n",
    "                'integration': 'Claude Sonnet 4'\n",
    "            },\n",
    "            'results': results,\n",
    "            'cost_breakdown': cost_breakdown,\n",
    "            'settings': {\n",
    "                'data_file': DATA_FILE,\n",
    "                'test_size': TEST_SIZE,\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'test_subset_size': len(test_subset),\n",
    "                'langchain_available': LANGCHAIN_AVAILABLE,\n",
    "                'rdkit_available': RDKIT_AVAILABLE\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        filename = 'hybrid_rag_results.json'\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Results saved to '{filename}'\")\n",
    "        \n",
    "        # ì„±ëŠ¥ ìš”ì•½\n",
    "        mae = results['metrics']['mae']\n",
    "        r2 = results['metrics']['r2']\n",
    "        within_10 = results['metrics']['within_10_pct']\n",
    "        avg_assay_weight = results['metrics']['avg_assay_weight']\n",
    "        avg_chem_weight = results['metrics']['avg_chemical_weight']\n",
    "        \n",
    "        print(f\"\\nğŸ† HYBRID SYSTEM PERFORMANCE SUMMARY:\")\n",
    "        print(f\"   MAE: {mae:.2f} | RÂ²: {r2:.3f} | Within Â±10: {within_10:.1f}%\")\n",
    "        print(f\"   Avg Context Weights: Assay={avg_assay_weight:.2f}, Chemical={avg_chem_weight:.2f}\")\n",
    "        \n",
    "        # ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬\n",
    "        print(f\"\\nğŸ”§ System Status:\")\n",
    "        if LANGCHAIN_AVAILABLE and RDKIT_AVAILABLE:\n",
    "            print(\"   âœ… Full Hybrid System: Both LangChain and RDKit operational!\")\n",
    "            print(\"   ğŸ¯ Optimal performance with dual similarity engines\")\n",
    "        elif RDKIT_AVAILABLE:\n",
    "            print(\"   âš ï¸ Partial System: RDKit operational, LangChain disabled\")\n",
    "            print(\"   ğŸ“ Install LangChain for assay similarity: pip install langchain sentence-transformers\")\n",
    "        elif LANGCHAIN_AVAILABLE:\n",
    "            print(\"   âš ï¸ Partial System: LangChain operational, RDKit disabled\")\n",
    "            print(\"   ğŸ§ª Install RDKit for chemical similarity: conda install -c conda-forge rdkit\")\n",
    "        else:\n",
    "            print(\"   âŒ Minimal System: Both engines disabled\")\n",
    "            print(\"   ğŸ“¦ Install dependencies for full functionality\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Hybrid RAG Analysis complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
