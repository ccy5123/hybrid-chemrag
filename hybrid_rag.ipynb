{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain sentence-transformers\n",
    "# !conda install -c conda-forge rdkit\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anthropic\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import logging\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# LangChain imports (로컬 임베딩 사용)\n",
    "try:\n",
    "    from langchain.vectorstores import FAISS\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.schema import Document\n",
    "    LANGCHAIN_AVAILABLE = True\n",
    "    print(\"✅ LangChain loaded successfully!\")\n",
    "except ImportError:\n",
    "    LANGCHAIN_AVAILABLE = False\n",
    "    print(\"⚠️ LangChain not available. Install with: pip install langchain sentence-transformers\")\n",
    "\n",
    "# RDKit imports for molecular similarity\n",
    "try:\n",
    "    from rdkit import Chem, DataStructs\n",
    "    from rdkit.Chem import AllChem, MACCSkeys, Descriptors\n",
    "    from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "    RDKIT_AVAILABLE = True\n",
    "    print(\"✅ RDKit loaded successfully!\")\n",
    "except ImportError:\n",
    "    RDKIT_AVAILABLE = False\n",
    "    print(\"⚠️ RDKit not available. Install with: conda install -c conda-forge rdkit\")\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataParser:\n",
    "    \"\"\"입력 텍스트를 실험 조건과 SMILES로 분리\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_input_text(input_text: str) -> Dict:\n",
    "        \"\"\"input_text를 구성 요소로 분리\"\"\"\n",
    "        try:\n",
    "            # SMILES 추출\n",
    "            smiles_pattern = r'SMILES:\\s*([^\\n\\r]+)'\n",
    "            smiles_match = re.search(smiles_pattern, input_text)\n",
    "            smiles = smiles_match.group(1).strip() if smiles_match else \"\"\n",
    "            \n",
    "            # Assay 이름 추출\n",
    "            assay_pattern = r'Assay:\\s*([^\\n\\r]+)'\n",
    "            assay_match = re.search(assay_pattern, input_text)\n",
    "            assay_name = assay_match.group(1).strip() if assay_match else \"\"\n",
    "            \n",
    "            # 실험 설명 추출 (TOX21... 로 시작하는 부분)\n",
    "            assay_desc_pattern = r'(TOX21_[^\\.]+[^\\.]*\\.)'\n",
    "            assay_desc_match = re.search(assay_desc_pattern, input_text, re.DOTALL)\n",
    "            assay_description = assay_desc_match.group(1).strip() if assay_desc_match else \"\"\n",
    "            \n",
    "            # 전체 지시사항 추출\n",
    "            instruction_pattern = r'(Given an Assay and SMILES.*?)(?:SMILES:|$)'\n",
    "            instruction_match = re.search(instruction_pattern, input_text, re.DOTALL)\n",
    "            instruction = instruction_match.group(1).strip() if instruction_match else \"\"\n",
    "            \n",
    "            return {\n",
    "                'smiles': smiles,\n",
    "                'assay_name': assay_name,\n",
    "                'assay_description': assay_description,\n",
    "                'instruction': instruction,\n",
    "                'full_text': input_text\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing input text: {e}\")\n",
    "            return {\n",
    "                'smiles': '',\n",
    "                'assay_name': '',\n",
    "                'assay_description': '',\n",
    "                'instruction': '',\n",
    "                'full_text': input_text\n",
    "            }\n",
    "\n",
    "class HybridSMILESRAG:\n",
    "    \"\"\"\n",
    "    하이브리드 RAG 시스템: 자연어(LangChain) + 화학적 유사도(RDKit)\n",
    "    \n",
    "    Features:\n",
    "    - 실험 조건 유사도: LangChain + 로컬 임베딩\n",
    "    - 화학적 유사도: RDKit 분자 지문\n",
    "    - 적응형 융합: 동적 가중치 기반 통합\n",
    "    - Claude 3.7 추론: 하이브리드 컨텍스트 활용\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, claude_api_key: str = None, model: str = \"claude-sonnet-4-20250514\", temperature: float = 0.1):\n",
    "        # Claude API 설정\n",
    "        self.claude_client = anthropic.Anthropic(\n",
    "            api_key=claude_api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        )\n",
    "        if not (claude_api_key or os.getenv(\"ANTHROPIC_API_KEY\")):\n",
    "            raise ValueError(\"Claude API key not found. Set ANTHROPIC_API_KEY environment variable.\")\n",
    "        \n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # 데이터 저장소\n",
    "        self.train_data = []\n",
    "        self.parsed_train_data = []\n",
    "        \n",
    "        # LangChain 컴포넌트 (로컬 임베딩 사용)\n",
    "        if LANGCHAIN_AVAILABLE:\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"all-MiniLM-L6-v2\",\n",
    "                model_kwargs={'device': 'cpu'}\n",
    "            )\n",
    "            self.assay_vectorstore = None\n",
    "            logger.info(\"🔤 Using local HuggingFace embeddings for assay similarity\")\n",
    "        else:\n",
    "            self.embeddings = None\n",
    "            logger.warning(\"⚠️ LangChain not available, assay similarity disabled\")\n",
    "        \n",
    "        # RDKit 컴포넌트\n",
    "        self.mol_objects = {}\n",
    "        self.fingerprints = {}\n",
    "        \n",
    "        # 비용 추적\n",
    "        self.cost_tracker = {\n",
    "            'input_tokens': 0,\n",
    "            'output_tokens': 0,\n",
    "            'total_cost': 0.0,\n",
    "            'api_calls': 0\n",
    "        }\n",
    "        \n",
    "        logger.info(\"🔬 Hybrid RAG System initialized\")\n",
    "    \n",
    "    def load_jsonl_data(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"JSONL 파일에서 데이터 로드\"\"\"\n",
    "        data = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line.strip())\n",
    "                    data.append(item)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "        \n",
    "        logger.info(f\"Loaded {len(data)} samples from {file_path}\")\n",
    "        return data\n",
    "    \n",
    "    def simple_train_test_split(self, data: List[Dict], test_size: float = 0.2, random_state: int = 42) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"간단한 train/test 분할\"\"\"\n",
    "        random.seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        shuffled_data = data.copy()\n",
    "        random.shuffle(shuffled_data)\n",
    "        \n",
    "        split_idx = int(len(shuffled_data) * (1 - test_size))\n",
    "        train_data = shuffled_data[:split_idx]\n",
    "        test_data = shuffled_data[split_idx:]\n",
    "        \n",
    "        logger.info(f\"Train set: {len(train_data)} samples\")\n",
    "        logger.info(f\"Test set: {len(test_data)} samples\")\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    def prepare_hybrid_training_data(self, train_data: List[Dict]):\n",
    "        \"\"\"하이브리드 훈련 데이터 준비\"\"\"\n",
    "        logger.info(\"🏗️ Preparing hybrid training data...\")\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.parsed_train_data = []\n",
    "        assay_documents = []\n",
    "        \n",
    "        for idx, item in enumerate(train_data):\n",
    "            # 1. 텍스트 파싱\n",
    "            parsed = DataParser.parse_input_text(item['input_text'])\n",
    "            parsed['logac50'] = int(item['output_text'])\n",
    "            parsed['idx'] = idx\n",
    "            \n",
    "            # 2. 화학 구조 처리 (RDKit)\n",
    "            if RDKIT_AVAILABLE and parsed['smiles']:\n",
    "                mol = self._create_mol_object(parsed['smiles'])\n",
    "                if mol is not None:\n",
    "                    parsed['mol'] = mol\n",
    "                    parsed['fingerprints'] = self._generate_fingerprints(mol, parsed['smiles'])\n",
    "                    parsed['molecular_props'] = self._calculate_molecular_properties(mol)\n",
    "                    parsed['activity_category'] = self._categorize_activity(parsed['logac50'])\n",
    "                    \n",
    "                    # 캐싱\n",
    "                    self.mol_objects[parsed['smiles']] = mol\n",
    "                    self.fingerprints[parsed['smiles']] = parsed['fingerprints']\n",
    "            \n",
    "            # 3. 실험 조건 문서 생성 (LangChain용)\n",
    "            if LANGCHAIN_AVAILABLE and parsed['assay_description']:\n",
    "                assay_doc_content = f\"\"\"\n",
    "                Assay: {parsed['assay_name']}\n",
    "                Description: {parsed['assay_description']}\n",
    "                Activity: {parsed['logac50']}\n",
    "                Category: {self._categorize_activity(parsed['logac50'])}\n",
    "                Instructions: {parsed['instruction']}\n",
    "                \"\"\"\n",
    "                \n",
    "                assay_doc = Document(\n",
    "                    page_content=assay_doc_content,\n",
    "                    metadata={\n",
    "                        'assay_name': parsed['assay_name'],\n",
    "                        'logac50': parsed['logac50'],\n",
    "                        'idx': idx\n",
    "                    }\n",
    "                )\n",
    "                assay_documents.append(assay_doc)\n",
    "            \n",
    "            self.parsed_train_data.append(parsed)\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                logger.info(f\"Processed {idx + 1}/{len(train_data)} samples...\")\n",
    "        \n",
    "        # 4. LangChain 벡터스토어 구축\n",
    "        if LANGCHAIN_AVAILABLE and assay_documents:\n",
    "            logger.info(\"🔤 Building assay vector store...\")\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=50\n",
    "            )\n",
    "            split_docs = text_splitter.split_documents(assay_documents)\n",
    "            self.assay_vectorstore = FAISS.from_documents(split_docs, self.embeddings)\n",
    "            logger.info(f\"✅ Built assay vector store with {len(split_docs)} chunks\")\n",
    "        \n",
    "        logger.info(f\"✅ Prepared {len(self.parsed_train_data)} training examples\")\n",
    "    \n",
    "    def _categorize_activity(self, logac50: int) -> str:\n",
    "        \"\"\"활성도 카테고리 분류\"\"\"\n",
    "        if logac50 >= 85:\n",
    "            return \"Very High\"\n",
    "        elif logac50 >= 70:\n",
    "            return \"High\"\n",
    "        elif logac50 >= 50:\n",
    "            return \"Medium\"\n",
    "        elif logac50 >= 30:\n",
    "            return \"Low\"\n",
    "        else:\n",
    "            return \"Very Low\"\n",
    "    \n",
    "    def _create_mol_object(self, smiles: str):\n",
    "        \"\"\"SMILES에서 RDKit 분자 객체 생성\"\"\"\n",
    "        if not RDKIT_AVAILABLE:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                mol = Chem.AddHs(mol)\n",
    "                return mol\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error creating mol object for {smiles}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _generate_fingerprints(self, mol, smiles: str) -> Dict:\n",
    "        \"\"\"다양한 분자 지문 생성\"\"\"\n",
    "        if not RDKIT_AVAILABLE or mol is None:\n",
    "            return {}\n",
    "        \n",
    "        fingerprints = {}\n",
    "        try:\n",
    "            fingerprints['morgan'] = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "            fingerprints['maccs'] = MACCSkeys.GenMACCSKeys(mol)\n",
    "            fingerprints['rdkit'] = AllChem.GetRDKitFPGenerator().GetFingerprint(mol)\n",
    "            fingerprints['atompair'] = AllChem.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=2048)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error generating fingerprints for {smiles}: {e}\")\n",
    "        \n",
    "        return fingerprints\n",
    "    \n",
    "    def _calculate_molecular_properties(self, mol) -> Dict:\n",
    "        \"\"\"분자 물성 계산\"\"\"\n",
    "        if not RDKIT_AVAILABLE or mol is None:\n",
    "            return {}\n",
    "            \n",
    "        try:\n",
    "            props = {\n",
    "                'mw': Descriptors.MolWt(mol),\n",
    "                'logp': Descriptors.MolLogP(mol),\n",
    "                'hbd': Descriptors.NumHDonors(mol),\n",
    "                'hba': Descriptors.NumHAcceptors(mol),\n",
    "                'tpsa': Descriptors.TPSA(mol),\n",
    "                'rotatable_bonds': Descriptors.NumRotatableBonds(mol),\n",
    "                'aromatic_rings': Descriptors.NumAromaticRings(mol),\n",
    "                'heavy_atoms': Descriptors.HeavyAtomCount(mol)\n",
    "            }\n",
    "            return props\n",
    "        except Exception:\n",
    "            return {}\n",
    "    \n",
    "    def hybrid_similarity_search(self, query_input: str, k_assay: int = 3, k_chemical: int = 5) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"하이브리드 유사도 검색: 실험조건 + 화학구조\"\"\"\n",
    "        \n",
    "        # 1. 입력 파싱\n",
    "        parsed_query = DataParser.parse_input_text(query_input)\n",
    "        \n",
    "        # 2. 실험 조건 유사도 검색 (LangChain)\n",
    "        similar_assays = []\n",
    "        if LANGCHAIN_AVAILABLE and self.assay_vectorstore and parsed_query['assay_description']:\n",
    "            try:\n",
    "                assay_query = f\"{parsed_query['assay_name']} {parsed_query['assay_description']}\"\n",
    "                assay_docs = self.assay_vectorstore.similarity_search_with_score(\n",
    "                    assay_query, k=k_assay\n",
    "                )\n",
    "                \n",
    "                for doc, score in assay_docs:\n",
    "                    similar_assays.append({\n",
    "                        'content': doc.page_content,\n",
    "                        'metadata': doc.metadata,\n",
    "                        'similarity_score': 1 - score,  # 거리를 유사도로 변환\n",
    "                        'assay_name': doc.metadata.get('assay_name', ''),\n",
    "                        'logac50': doc.metadata.get('logac50', 0)\n",
    "                    })\n",
    "                    \n",
    "                logger.debug(f\"Found {len(similar_assays)} similar assays\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Assay search failed: {e}\")\n",
    "        \n",
    "        # 3. 화학적 유사도 검색 (RDKit)\n",
    "        similar_molecules = []\n",
    "        if RDKIT_AVAILABLE and parsed_query['smiles']:\n",
    "            try:\n",
    "                query_mol = self._create_mol_object(parsed_query['smiles'])\n",
    "                if query_mol is not None:\n",
    "                    query_fps = self._generate_fingerprints(query_mol, parsed_query['smiles'])\n",
    "                    \n",
    "                    similarities = []\n",
    "                    for example in self.parsed_train_data:\n",
    "                        if 'fingerprints' in example and example['fingerprints']:\n",
    "                            similarity_scores = self._calculate_multi_fingerprint_similarity(\n",
    "                                query_fps, example['fingerprints']\n",
    "                            )\n",
    "                            final_similarity = self._combine_similarity_scores(similarity_scores)\n",
    "                            \n",
    "                            similarities.append((final_similarity, example, similarity_scores))\n",
    "                    \n",
    "                    # 상위 k개 선택\n",
    "                    similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "                    \n",
    "                    for sim_score, example, breakdown in similarities[:k_chemical]:\n",
    "                        similar_molecules.append({\n",
    "                            'smiles': example['smiles'],\n",
    "                            'logac50': example['logac50'],\n",
    "                            'activity_category': example.get('activity_category', ''),\n",
    "                            'molecular_props': example.get('molecular_props', {}),\n",
    "                            'similarity_score': sim_score,\n",
    "                            'similarity_breakdown': breakdown,\n",
    "                            'assay_name': example.get('assay_name', '')\n",
    "                        })\n",
    "                    \n",
    "                    logger.debug(f\"Found {len(similar_molecules)} similar molecules\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Chemical search failed: {e}\")\n",
    "        \n",
    "        return similar_assays, similar_molecules\n",
    "    \n",
    "    def _calculate_multi_fingerprint_similarity(self, query_fps: Dict, target_fps: Dict) -> Dict:\n",
    "        \"\"\"다중 지문을 사용한 유사도 계산\"\"\"\n",
    "        similarities = {}\n",
    "        \n",
    "        fingerprint_types = ['morgan', 'maccs', 'rdkit', 'atompair']\n",
    "        \n",
    "        for fp_type in fingerprint_types:\n",
    "            if fp_type in query_fps and fp_type in target_fps:\n",
    "                try:\n",
    "                    tanimoto_sim = DataStructs.TanimotoSimilarity(\n",
    "                        query_fps[fp_type], target_fps[fp_type]\n",
    "                    )\n",
    "                    similarities[fp_type] = tanimoto_sim\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error calculating {fp_type} similarity: {e}\")\n",
    "                    similarities[fp_type] = 0.0\n",
    "            else:\n",
    "                similarities[fp_type] = 0.0\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def _combine_similarity_scores(self, similarity_scores: Dict) -> float:\n",
    "        \"\"\"여러 지문 유사도를 가중 평균으로 결합\"\"\"\n",
    "        weights = {\n",
    "            'morgan': 0.4,\n",
    "            'maccs': 0.3,\n",
    "            'rdkit': 0.2,\n",
    "            'atompair': 0.1\n",
    "        }\n",
    "        \n",
    "        weighted_sum = 0.0\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        for fp_type, weight in weights.items():\n",
    "            if fp_type in similarity_scores:\n",
    "                weighted_sum += similarity_scores[fp_type] * weight\n",
    "                total_weight += weight\n",
    "        \n",
    "        return weighted_sum / total_weight if total_weight > 0 else 0.0\n",
    "    \n",
    "    def calculate_context_weights(self, similar_assays: List[Dict], similar_molecules: List[Dict]) -> Dict:\n",
    "        \"\"\"컨텍스트 가중치 동적 계산\"\"\"\n",
    "        \n",
    "        # 최고 유사도 점수 추출\n",
    "        max_assay_sim = max([assay.get('similarity_score', 0) for assay in similar_assays]) if similar_assays else 0\n",
    "        max_chem_sim = max([mol.get('similarity_score', 0) for mol in similar_molecules]) if similar_molecules else 0\n",
    "        \n",
    "        # 데이터 가용성 고려\n",
    "        assay_availability = len(similar_assays) / 3.0  # 최대 3개 대비\n",
    "        chem_availability = len(similar_molecules) / 5.0  # 최대 5개 대비\n",
    "        \n",
    "        # 동적 가중치 계산\n",
    "        if max_assay_sim > 0.8 and max_chem_sim < 0.5:\n",
    "            # 실험 조건 매우 유사, 화학 구조 다름\n",
    "            weights = {'assay': 0.75, 'chemical': 0.25}\n",
    "        elif max_chem_sim > 0.8 and max_assay_sim < 0.5:\n",
    "            # 화학 구조 매우 유사, 실험 조건 다름\n",
    "            weights = {'assay': 0.25, 'chemical': 0.75}\n",
    "        elif max_assay_sim > 0.7 and max_chem_sim > 0.7:\n",
    "            # 둘 다 유사함 - 균형\n",
    "            weights = {'assay': 0.5, 'chemical': 0.5}\n",
    "        else:\n",
    "            # 기본 가중치, 가용성으로 조정\n",
    "            base_assay_weight = 0.4 + (assay_availability * 0.2)\n",
    "            base_chem_weight = 0.6 - (assay_availability * 0.2)\n",
    "            weights = {'assay': base_assay_weight, 'chemical': base_chem_weight}\n",
    "        \n",
    "        # 정규화\n",
    "        total = weights['assay'] + weights['chemical']\n",
    "        weights = {k: v/total for k, v in weights.items()}\n",
    "        \n",
    "        logger.debug(f\"Context weights: Assay={weights['assay']:.2f}, Chemical={weights['chemical']:.2f}\")\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def create_hybrid_prompt(self, query_input: str, similar_assays: List[Dict], similar_molecules: List[Dict], weights: Dict) -> str:\n",
    "        \"\"\"하이브리드 컨텍스트 통합 프롬프트 생성\"\"\"\n",
    "        \n",
    "        parsed_query = DataParser.parse_input_text(query_input)\n",
    "        \n",
    "        # 실험 조건 컨텍스트\n",
    "        assay_context = \"\"\n",
    "        if similar_assays:\n",
    "            assay_context = f\"🧪 EXPERIMENTAL PROTOCOL CONTEXT (Weight: {weights['assay']:.2f}):\\n\\n\"\n",
    "            for i, assay in enumerate(similar_assays, 1):\n",
    "                assay_context += f\"Similar Assay {i} (Similarity: {assay['similarity_score']:.3f}):\\n\"\n",
    "                assay_context += f\"  {assay['content']}\\n\\n\"\n",
    "        else:\n",
    "            assay_context = \"🧪 EXPERIMENTAL PROTOCOL CONTEXT: No similar assays found.\\n\\n\"\n",
    "        \n",
    "        # 화학 구조 컨텍스트\n",
    "        chemical_context = \"\"\n",
    "        if similar_molecules:\n",
    "            chemical_context = f\"🧬 CHEMICAL STRUCTURE CONTEXT (Weight: {weights['chemical']:.2f}):\\n\\n\"\n",
    "            for i, mol in enumerate(similar_molecules, 1):\n",
    "                chemical_context += f\"Similar Molecule {i} (Tanimoto: {mol['similarity_score']:.3f}):\\n\"\n",
    "                chemical_context += f\"  SMILES: {mol['smiles']}\\n\"\n",
    "                chemical_context += f\"  LogAC50: {mol['logac50']}\\n\"\n",
    "                chemical_context += f\"  Activity: {mol['activity_category']}\\n\"\n",
    "                \n",
    "                if 'similarity_breakdown' in mol:\n",
    "                    breakdown = mol['similarity_breakdown']\n",
    "                    chemical_context += f\"  Fingerprint Details:\\n\"\n",
    "                    chemical_context += f\"    - Morgan: {breakdown.get('morgan', 0):.3f}\\n\"\n",
    "                    chemical_context += f\"    - MACCS: {breakdown.get('maccs', 0):.3f}\\n\"\n",
    "                    chemical_context += f\"    - RDKit: {breakdown.get('rdkit', 0):.3f}\\n\"\n",
    "                \n",
    "                if 'molecular_props' in mol and mol['molecular_props']:\n",
    "                    props = mol['molecular_props']\n",
    "                    chemical_context += f\"  Properties: MW={props.get('mw', 'N/A'):.1f}, \"\n",
    "                    chemical_context += f\"LogP={props.get('logp', 'N/A'):.2f}\\n\"\n",
    "                \n",
    "                chemical_context += \"\\n\"\n",
    "        else:\n",
    "            chemical_context = \"🧬 CHEMICAL STRUCTURE CONTEXT: No similar molecules found.\\n\\n\"\n",
    "        \n",
    "        # 통합 프롬프트\n",
    "        integrated_prompt = f\"\"\"<thinking>\n",
    "I am performing a hybrid analysis combining experimental protocol knowledge and chemical structure similarity for toxicity prediction.\n",
    "\n",
    "Query Details:\n",
    "- Assay: {parsed_query['assay_name']}\n",
    "- SMILES: {parsed_query['smiles']}\n",
    "\n",
    "Context Analysis:\n",
    "- Assay context weight: {weights['assay']:.2f}\n",
    "- Chemical context weight: {weights['chemical']:.2f}\n",
    "\n",
    "This weighting suggests I should prioritize {\"experimental context\" if weights['assay'] > weights['chemical'] else \"chemical structure analysis\"} while considering both sources of information.\n",
    "\n",
    "Let me analyze the patterns systematically...\n",
    "</thinking>\n",
    "\n",
    "{assay_context}\n",
    "\n",
    "{chemical_context}\n",
    "\n",
    "🎯 HYBRID TOXICITY PREDICTION TASK:\n",
    "\n",
    "Query Input:\n",
    "- Assay: {parsed_query['assay_name']}\n",
    "- SMILES: {parsed_query['smiles']}\n",
    "- Task: {parsed_query['instruction']}\n",
    "\n",
    "📊 INTEGRATED ANALYSIS FRAMEWORK:\n",
    "\n",
    "1. **Context Weighting Strategy**:\n",
    "   - Experimental Protocol Weight: {weights['assay']:.2f}\n",
    "   - Chemical Structure Weight: {weights['chemical']:.2f}\n",
    "\n",
    "2. **Primary Analysis Focus**:\n",
    "   {\"Focus on experimental protocol patterns and assay-specific factors\" if weights['assay'] > 0.6 else \"Focus on chemical structure-activity relationships\" if weights['chemical'] > 0.6 else \"Balance both experimental and chemical contexts equally\"}\n",
    "\n",
    "3. **Cross-Validation Approach**:\n",
    "   - Compare patterns from both experimental and chemical contexts\n",
    "   - Identify consistent vs conflicting predictions\n",
    "   - Resolve conflicts using the higher-weighted context\n",
    "\n",
    "4. **Evidence Integration**:\n",
    "   - Experimental evidence: {\"Strong\" if len(similar_assays) >= 2 else \"Moderate\" if len(similar_assays) == 1 else \"Weak\"}\n",
    "   - Chemical evidence: {\"Strong\" if len(similar_molecules) >= 3 else \"Moderate\" if len(similar_molecules) >= 1 else \"Weak\"}\n",
    "\n",
    "🔬 REQUIRED ANALYSIS:\n",
    "\n",
    "**EXPERIMENTAL CONTEXT ANALYSIS**:\n",
    "[Analyze the experimental protocol patterns and assay-specific factors]\n",
    "\n",
    "**CHEMICAL STRUCTURE ANALYSIS**:\n",
    "[Analyze the molecular structure and chemical similarity patterns]\n",
    "\n",
    "**INTEGRATED PREDICTION LOGIC**:\n",
    "[Combine both contexts using the calculated weights]\n",
    "\n",
    "**FINAL PREDICTION**: [INTEGER 0-100]\n",
    "\n",
    "**CONFIDENCE ASSESSMENT**: [High/Medium/Low with justification based on context quality]\n",
    "\n",
    "Remember: Weight your analysis according to the calculated context weights, but always provide reasoning from both experimental and chemical perspectives when available.\"\"\"\n",
    "\n",
    "        return integrated_prompt\n",
    "    \n",
    "    def predict_single(self, query_input: str) -> Tuple[int, str, float, Dict]:\n",
    "        \"\"\"단일 입력에 대한 하이브리드 예측\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 1. 하이브리드 유사도 검색\n",
    "            similar_assays, similar_molecules = self.hybrid_similarity_search(query_input)\n",
    "            \n",
    "            # 2. 컨텍스트 가중치 계산\n",
    "            weights = self.calculate_context_weights(similar_assays, similar_molecules)\n",
    "            \n",
    "            # 3. 통합 프롬프트 생성\n",
    "            prompt = self.create_hybrid_prompt(query_input, similar_assays, similar_molecules, weights)\n",
    "            \n",
    "            # 4. Claude API 호출\n",
    "            response = self.claude_client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=3000,  # 하이브리드 분석을 위한 더 긴 응답\n",
    "                temperature=self.temperature,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            result_text = response.content[0].text\n",
    "            \n",
    "            # 5. 토큰 사용량 추적\n",
    "            self.cost_tracker['input_tokens'] += response.usage.input_tokens\n",
    "            self.cost_tracker['output_tokens'] += response.usage.output_tokens\n",
    "            self.cost_tracker['api_calls'] += 1\n",
    "            \n",
    "            # 6. 예측값 추출\n",
    "            prediction = self._extract_prediction(result_text)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            # 7. 메타데이터 수집\n",
    "            metadata = {\n",
    "                'weights': weights,\n",
    "                'n_similar_assays': len(similar_assays),\n",
    "                'n_similar_molecules': len(similar_molecules),\n",
    "                'max_assay_similarity': max([a.get('similarity_score', 0) for a in similar_assays]) if similar_assays else 0,\n",
    "                'max_chemical_similarity': max([m.get('similarity_score', 0) for m in similar_molecules]) if similar_molecules else 0\n",
    "            }\n",
    "            \n",
    "            logger.debug(f\"Hybrid prediction: {prediction} (assay_weight: {weights['assay']:.2f}, chem_weight: {weights['chemical']:.2f})\")\n",
    "            \n",
    "            return prediction, result_text, elapsed_time, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in hybrid prediction: {e}\")\n",
    "            return 50, f\"Error: {str(e)}\", 0.0, {}\n",
    "    \n",
    "    def _extract_prediction(self, result_text: str) -> int:\n",
    "        \"\"\"LLM 응답에서 예측값 추출\"\"\"\n",
    "        patterns = [\n",
    "            r'FINAL PREDICTION:\\s*(\\d{1,3})',\n",
    "            r'PREDICTION:\\s*(\\d{1,3})',\n",
    "            r'Prediction:\\s*(\\d{1,3})',\n",
    "            r'LogAC50:\\s*(\\d{1,3})',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, result_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                val = int(match.group(1))\n",
    "                if 0 <= val <= 100:\n",
    "                    return val\n",
    "        \n",
    "        # 백업: 0-100 범위의 첫 번째 숫자\n",
    "        numbers = re.findall(r'\\b(\\d{1,3})\\b', result_text)\n",
    "        for num in numbers:\n",
    "            val = int(num)\n",
    "            if 0 <= val <= 100:\n",
    "                return val\n",
    "        \n",
    "        logger.warning(\"Could not extract valid prediction, using default value 50\")\n",
    "        return 50\n",
    "    \n",
    "    def evaluate_test_set(self, test_data: List[Dict]) -> Dict:\n",
    "        \"\"\"하이브리드 시스템으로 테스트 세트 평가\"\"\"\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        explanations = []\n",
    "        times = []\n",
    "        errors = []\n",
    "        metadata_list = []\n",
    "        \n",
    "        logger.info(f\"🔬 Starting hybrid evaluation on {len(test_data)} test samples...\")\n",
    "        \n",
    "        for i, item in enumerate(test_data):\n",
    "            input_text = item['input_text']\n",
    "            actual = int(item['output_text'])\n",
    "            \n",
    "            try:\n",
    "                pred, explanation, elapsed_time, metadata = self.predict_single(input_text)\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                actuals.append(actual)\n",
    "                explanations.append(explanation)\n",
    "                times.append(elapsed_time)\n",
    "                errors.append(None)\n",
    "                metadata_list.append(metadata)\n",
    "                \n",
    "                # 실시간 성능 모니터링\n",
    "                if i > 0:\n",
    "                    current_mae = np.mean([abs(a - p) for a, p in zip(actuals, predictions)])\n",
    "                    logger.info(f\"Sample {i+1}/{len(test_data)} | Pred: {pred} | Actual: {actual} | \"\n",
    "                              f\"Weights: A={metadata['weights']['assay']:.2f}/C={metadata['weights']['chemical']:.2f} | \"\n",
    "                              f\"Running MAE: {current_mae:.2f}\")\n",
    "                else:\n",
    "                    logger.info(f\"Sample {i+1}/{len(test_data)} | Pred: {pred} | Actual: {actual}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing sample {i}: {e}\")\n",
    "                predictions.append(50)\n",
    "                actuals.append(actual)\n",
    "                explanations.append(f\"Error: {str(e)}\")\n",
    "                times.append(0.0)\n",
    "                errors.append(str(e))\n",
    "                metadata_list.append({})\n",
    "        \n",
    "        # 성능 메트릭 계산\n",
    "        mae = np.mean([abs(a - p) for a, p in zip(actuals, predictions)])\n",
    "        mse = np.mean([(a - p)**2 for a, p in zip(actuals, predictions)])\n",
    "        rmse = math.sqrt(mse)\n",
    "        r2 = self._calculate_r2(actuals, predictions)\n",
    "        \n",
    "        # 하이브리드 특화 메트릭\n",
    "        assay_weighted_samples = [i for i, meta in enumerate(metadata_list) \n",
    "                                 if meta.get('weights', {}).get('assay', 0) > 0.6]\n",
    "        chemical_weighted_samples = [i for i, meta in enumerate(metadata_list) \n",
    "                                   if meta.get('weights', {}).get('chemical', 0) > 0.6]\n",
    "        balanced_samples = [i for i, meta in enumerate(metadata_list) \n",
    "                           if 0.4 <= meta.get('weights', {}).get('assay', 0.5) <= 0.6]\n",
    "        \n",
    "        # 가중치별 성능 분석\n",
    "        assay_mae = np.mean([abs(actuals[i] - predictions[i]) for i in assay_weighted_samples]) if assay_weighted_samples else float('inf')\n",
    "        chemical_mae = np.mean([abs(actuals[i] - predictions[i]) for i in chemical_weighted_samples]) if chemical_weighted_samples else float('inf')\n",
    "        balanced_mae = np.mean([abs(actuals[i] - predictions[i]) for i in balanced_samples]) if balanced_samples else float('inf')\n",
    "        \n",
    "        # 유사도 기반 분석\n",
    "        high_assay_sim_samples = [i for i, meta in enumerate(metadata_list) \n",
    "                                 if meta.get('max_assay_similarity', 0) > 0.7]\n",
    "        high_chem_sim_samples = [i for i, meta in enumerate(metadata_list) \n",
    "                                if meta.get('max_chemical_similarity', 0) > 0.7]\n",
    "        \n",
    "        results = {\n",
    "            'predictions': predictions,\n",
    "            'actuals': actuals,\n",
    "            'explanations': explanations,\n",
    "            'times': times,\n",
    "            'errors': errors,\n",
    "            'metadata': metadata_list,\n",
    "            'metrics': {\n",
    "                'mae': mae,\n",
    "                'mse': mse,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'within_10_pct': sum([1 for a, p in zip(actuals, predictions) if abs(a - p) <= 10]) / len(actuals) * 100,\n",
    "                'within_20_pct': sum([1 for a, p in zip(actuals, predictions) if abs(a - p) <= 20]) / len(actuals) * 100,\n",
    "                'assay_weighted_mae': assay_mae,\n",
    "                'chemical_weighted_mae': chemical_mae,\n",
    "                'balanced_mae': balanced_mae,\n",
    "                'n_assay_weighted': len(assay_weighted_samples),\n",
    "                'n_chemical_weighted': len(chemical_weighted_samples),\n",
    "                'n_balanced': len(balanced_samples),\n",
    "                'n_high_assay_sim': len(high_assay_sim_samples),\n",
    "                'n_high_chem_sim': len(high_chem_sim_samples),\n",
    "                'avg_assay_weight': np.mean([meta.get('weights', {}).get('assay', 0.5) for meta in metadata_list]),\n",
    "                'avg_chemical_weight': np.mean([meta.get('weights', {}).get('chemical', 0.5) for meta in metadata_list]),\n",
    "                'n_samples': len(test_data),\n",
    "                'n_errors': len([e for e in errors if e is not None]),\n",
    "                'avg_time': np.mean([t for t in times if t > 0])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_r2(self, y_true: List[float], y_pred: List[float]) -> float:\n",
    "        \"\"\"R² Score 계산\"\"\"\n",
    "        y_mean = np.mean(y_true)\n",
    "        ss_tot = sum([(y - y_mean)**2 for y in y_true])\n",
    "        ss_res = sum([(t - p)**2 for t, p in zip(y_true, y_pred)])\n",
    "        return 1 - (ss_res / ss_tot) if ss_tot != 0 else 0.0\n",
    "    \n",
    "    def calculate_cost(self) -> Dict:\n",
    "        \"\"\"Claude API 비용 계산\"\"\"\n",
    "        input_cost_per_1k = 0.003   # $3/1M = $0.003/1K\n",
    "        output_cost_per_1k = 0.015  # $15/1M = $0.015/1K\n",
    "        \n",
    "        input_cost = (self.cost_tracker['input_tokens'] / 1000) * input_cost_per_1k\n",
    "        output_cost = (self.cost_tracker['output_tokens'] / 1000) * output_cost_per_1k\n",
    "        total_cost = input_cost + output_cost\n",
    "        \n",
    "        return {\n",
    "            'input_tokens': self.cost_tracker['input_tokens'],\n",
    "            'output_tokens': self.cost_tracker['output_tokens'],\n",
    "            'api_calls': self.cost_tracker['api_calls'],\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': total_cost\n",
    "        }\n",
    "    \n",
    "    def print_results(self, results: Dict, cost_breakdown: Dict):\n",
    "        \"\"\"하이브리드 시스템 결과 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"🔬 HYBRID RAG SYSTEM - SMILES TOXICITY PREDICTION RESULTS\")\n",
    "        print(\"=\"*90)\n",
    "        \n",
    "        metrics = results['metrics']\n",
    "        \n",
    "        print(f\"\\n📊 Overall Performance Metrics:\")\n",
    "        print(f\"   • Mean Absolute Error (MAE): {metrics['mae']:.2f}\")\n",
    "        print(f\"   • Root Mean Square Error (RMSE): {metrics['rmse']:.2f}\")\n",
    "        print(f\"   • R² Score: {metrics['r2']:.3f}\")\n",
    "        print(f\"   • Accuracy within ±10: {metrics['within_10_pct']:.1f}%\")\n",
    "        print(f\"   • Accuracy within ±20: {metrics['within_20_pct']:.1f}%\")\n",
    "        \n",
    "        print(f\"\\n🔀 Hybrid Analysis Breakdown:\")\n",
    "        print(f\"   • Average Assay Weight: {metrics['avg_assay_weight']:.2f}\")\n",
    "        print(f\"   • Average Chemical Weight: {metrics['avg_chemical_weight']:.2f}\")\n",
    "        print(f\"   • Assay-Weighted Samples: {metrics['n_assay_weighted']} (MAE: {metrics['assay_weighted_mae']:.2f})\")\n",
    "        print(f\"   • Chemical-Weighted Samples: {metrics['n_chemical_weighted']} (MAE: {metrics['chemical_weighted_mae']:.2f})\")\n",
    "        print(f\"   • Balanced Samples: {metrics['n_balanced']} (MAE: {metrics['balanced_mae']:.2f})\")\n",
    "        \n",
    "        print(f\"\\n🎯 Similarity Analysis:\")\n",
    "        print(f\"   • High Assay Similarity (>0.7): {metrics['n_high_assay_sim']} samples\")\n",
    "        print(f\"   • High Chemical Similarity (>0.7): {metrics['n_high_chem_sim']} samples\")\n",
    "        \n",
    "        print(f\"\\n⚡ Performance:\")\n",
    "        print(f\"   • Average Prediction Time: {metrics['avg_time']:.2f}s\")\n",
    "        print(f\"   • Test Samples: {metrics['n_samples']}\")\n",
    "        print(f\"   • Errors: {metrics['n_errors']}\")\n",
    "        \n",
    "        print(f\"\\n💰 Cost Breakdown (Claude Sonnet 4):\")\n",
    "        print(f\"   • API Calls: {cost_breakdown['api_calls']}\")\n",
    "        print(f\"   • Input Tokens: {cost_breakdown['input_tokens']:,}\")\n",
    "        print(f\"   • Output Tokens: {cost_breakdown['output_tokens']:,}\")\n",
    "        print(f\"   • Total Cost: ${cost_breakdown['total_cost']:.4f}\")\n",
    "        \n",
    "        # 최고/최악 예측 분석\n",
    "        errors = [abs(a - p) for a, p in zip(results['actuals'], results['predictions'])]\n",
    "        best_indices = np.argsort(errors)[:3]\n",
    "        worst_indices = np.argsort(errors)[-3:][::-1]\n",
    "        \n",
    "        print(f\"\\n🏆 Best Hybrid Predictions:\")\n",
    "        for i, idx in enumerate(best_indices):\n",
    "            actual = results['actuals'][idx]\n",
    "            pred = results['predictions'][idx]\n",
    "            metadata = results['metadata'][idx]\n",
    "            error = abs(actual - pred)\n",
    "            weights = metadata.get('weights', {})\n",
    "            print(f\"   {i+1}. Actual: {actual:2d}, Predicted: {pred:2d}, Error: {error:2d}, \"\n",
    "                  f\"Weights: A={weights.get('assay', 0):.2f}/C={weights.get('chemical', 0):.2f}\")\n",
    "        \n",
    "        print(f\"\\n🤔 Most Challenging Predictions:\")\n",
    "        for i, idx in enumerate(worst_indices):\n",
    "            actual = results['actuals'][idx]\n",
    "            pred = results['predictions'][idx]\n",
    "            metadata = results['metadata'][idx]\n",
    "            error = abs(actual - pred)\n",
    "            weights = metadata.get('weights', {})\n",
    "            print(f\"   {i+1}. Actual: {actual:2d}, Predicted: {pred:2d}, Error: {error:2d}, \"\n",
    "                  f\"Weights: A={weights.get('assay', 0):.2f}/C={weights.get('chemical', 0):.2f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"하이브리드 RAG 시스템 메인 실행 함수\"\"\"\n",
    "    DATA_FILE = \"combined_train_sampled2.jsonl\"\n",
    "    TEST_SIZE = 0.2\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # API 키 설정\n",
    "    CLAUDE_API_KEY = \"\"  # 실제 키로 교체하세요\n",
    "    \n",
    "    try:\n",
    "        print(\"🔬 Initializing Hybrid RAG System...\")\n",
    "        print(\"   • Natural Language Processing: LangChain + Local Embeddings\")\n",
    "        print(\"   • Chemical Similarity: RDKit + Molecular Fingerprints\")\n",
    "        print(\"   • Integration: Claude Sonnet 4\")\n",
    "        \n",
    "        predictor = HybridSMILESRAG(claude_api_key=CLAUDE_API_KEY)\n",
    "        \n",
    "        print(\"\\n📚 Loading data...\")\n",
    "        data = predictor.load_jsonl_data(DATA_FILE)\n",
    "        \n",
    "        print(\"✂️ Splitting data...\")\n",
    "        train_data, test_data = predictor.simple_train_test_split(\n",
    "            data, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        print(\"🏗️ Preparing hybrid training data...\")\n",
    "        print(\"   • Parsing experimental conditions and SMILES\")\n",
    "        print(\"   • Building assay vector store (LangChain)\")\n",
    "        print(\"   • Generating molecular fingerprints (RDKit)\")\n",
    "        predictor.prepare_hybrid_training_data(train_data)\n",
    "        \n",
    "        print(\"\\n🔬 Evaluating with Hybrid RAG System...\")\n",
    "        print(\"   • Experimental similarity search + Chemical similarity search\")\n",
    "        print(\"   • Dynamic context weighting + Integrated reasoning\")\n",
    "        \n",
    "        # 하이브리드 시스템 테스트\n",
    "        test_subset = test_data[:20]  # 비용 고려하여 20개로 시작\n",
    "        results = predictor.evaluate_test_set(test_subset)\n",
    "        \n",
    "        print(\"\\n💰 Calculating costs...\")\n",
    "        cost_breakdown = predictor.calculate_cost()\n",
    "        \n",
    "        print(\"\\n📊 Generating comprehensive results...\")\n",
    "        predictor.print_results(results, cost_breakdown)\n",
    "        \n",
    "        # 상세 결과 저장\n",
    "        output_data = {\n",
    "            'model': 'hybrid-rag-claude-sonnet-4',\n",
    "            'components': {\n",
    "                'assay_similarity': 'LangChain + HuggingFace Embeddings' if LANGCHAIN_AVAILABLE else 'Disabled',\n",
    "                'chemical_similarity': 'RDKit + Molecular Fingerprints' if RDKIT_AVAILABLE else 'String Fallback',\n",
    "                'integration': 'Claude Sonnet 4'\n",
    "            },\n",
    "            'results': results,\n",
    "            'cost_breakdown': cost_breakdown,\n",
    "            'settings': {\n",
    "                'data_file': DATA_FILE,\n",
    "                'test_size': TEST_SIZE,\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'test_subset_size': len(test_subset),\n",
    "                'langchain_available': LANGCHAIN_AVAILABLE,\n",
    "                'rdkit_available': RDKIT_AVAILABLE\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        filename = 'hybrid_rag_results.json'\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n💾 Results saved to '{filename}'\")\n",
    "        \n",
    "        # 성능 요약\n",
    "        mae = results['metrics']['mae']\n",
    "        r2 = results['metrics']['r2']\n",
    "        within_10 = results['metrics']['within_10_pct']\n",
    "        avg_assay_weight = results['metrics']['avg_assay_weight']\n",
    "        avg_chem_weight = results['metrics']['avg_chemical_weight']\n",
    "        \n",
    "        print(f\"\\n🏆 HYBRID SYSTEM PERFORMANCE SUMMARY:\")\n",
    "        print(f\"   MAE: {mae:.2f} | R²: {r2:.3f} | Within ±10: {within_10:.1f}%\")\n",
    "        print(f\"   Avg Context Weights: Assay={avg_assay_weight:.2f}, Chemical={avg_chem_weight:.2f}\")\n",
    "        \n",
    "        # 시스템 상태 체크\n",
    "        print(f\"\\n🔧 System Status:\")\n",
    "        if LANGCHAIN_AVAILABLE and RDKIT_AVAILABLE:\n",
    "            print(\"   ✅ Full Hybrid System: Both LangChain and RDKit operational!\")\n",
    "            print(\"   🎯 Optimal performance with dual similarity engines\")\n",
    "        elif RDKIT_AVAILABLE:\n",
    "            print(\"   ⚠️ Partial System: RDKit operational, LangChain disabled\")\n",
    "            print(\"   📝 Install LangChain for assay similarity: pip install langchain sentence-transformers\")\n",
    "        elif LANGCHAIN_AVAILABLE:\n",
    "            print(\"   ⚠️ Partial System: LangChain operational, RDKit disabled\")\n",
    "            print(\"   🧪 Install RDKit for chemical similarity: conda install -c conda-forge rdkit\")\n",
    "        else:\n",
    "            print(\"   ❌ Minimal System: Both engines disabled\")\n",
    "            print(\"   📦 Install dependencies for full functionality\")\n",
    "        \n",
    "        print(f\"\\n🎉 Hybrid RAG Analysis complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
